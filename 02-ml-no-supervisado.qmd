# Aprendizaje máquina no supervisado {#sec-unsupervised-ml}

## Clustering

El análisis *cluster* (agrupamiento) identifica grupos de objetos basándose únicamente en la
información que proporcionan los datos que describen esos objetos y sus relaciones entre sí.
El **objetivo** es que los objetos asignados al mismo grupo se similares o estén relacionados
entre sí y, a su vez, sean distintos (o no estén relacionados) con los elementos en otros
grupos. En consecuencia, cuanto mayor sea la **homogeneidad** dentro de un mismo grupo y
mayor sea la diferencia entre grupos, mejor o más distintivo será el *clustering*
[@tan2019].

Sin embargo, identificar los posibles agrupamientos en un conjunto de datos no es tan sencillo 
como puede parecer a primera vista. La @fig-four-clusters muestra un conjunto de tan sólo 20 elementos
en el que podemos formar, al menos, tres posibles agrupamientos de 2, 4 y 6 grupos, respectivamente.
¿Cuál es la mejor opción? En muchos casos, no sólo depende de las métricas de evaluación que utilicemos,
sino también de las descripciones disponibles sobre los datos y del conocimiento del dominio de
aplicación del problema.

![Un conjunto de 20 elmentos y cuatro posibles agrupamientos, con distinto número de grupos en cada caso. Basado en la Fig. 5.1 de [@tan2019].](img/four-clusters.png){#fig-four-clusters width="95%"}

Existen diferentes tipos de algoritmos de agrupamiento o *clustering*, en función de las propiedades
de los grupos que obtenemos como resultado del proceso.

- **Clústering jerárquico** (o anidado): en este caso el resultado son agrupamientos anidados, unos
dentro de los otros, que conforman una jerarquía de grupos. Según la estrategia empleada para crear
los grupos, pueden ser:
  - *Aglomerativos*: se parte de un primer grupo de datos, y se van uniendo progresivamente los siguientes
  grupos de datos más similares, hasta que todos los datos quedan en un único grupo a alto nivel.
  Normalmente, este algoritmo recibe el nombre de AGNES (AGlomerative NESting) [@boehmke2019]. Es mejor
  detectando grupos de pequeño tamaño, con pocos elementos.
  - *Divisivos*: se comienza considerando a todos los datos dentro de un mismo grupo, y progresivamente
  se van dividiendo en grupos anidados, de modo que los elementos más cercanos queden en el mismo grupo.
  Este algoritmo se denomina con frecuencia DIANA (DIvise ANAlysis). Es más adecuado para detectar grupos
  de gran tamaño, con muchos elementos.
- **Clústering partitivo (no jerárquico)**: propone una división sin solape entre los grupos de elementos,
de forma que cada grupo contenga elementos parecidos entre sí. Lo normal es que sigan un procedimiento
iterativo para resolver el problema, cuya solución exacta no se puede hallar computacionalmente.

En función de si existe o no solape entre los grupos tenemos:

- **Clustering sin solape**: es el tipo de clustering propuesto por los algoritmos partitivos. También hay
que considerar el clustering jerárquico como sin solape, puesto que la relación entre agrupamientos anidados
se produce a distintos niveles (no es lo mismo un solape entre dos grupos al mismo nivel que el hecho de que
un grupo esté contenido en otro de mayor nivel jerárquico).
- **Clustering con solape** (no exclusivo): permite que un elemento pueda pertenecer a más de un grupo
a la vez. Es decir, pueden existir solapes entre las regiones definidas para cada grupo.
- **Clustering difuso** (*fuzzy clustering*): todo objeto pertenece a todos los agrupamientos con un cierto
grado (peso) de pertenencia entre 0 (no pertenece en absoluto) y 1 (pertenece con toda seguridad).
Usualmente, se impone la condición adicional de que la suma de los pesos asignados a un elemento sea 1.
En el *clustering probabilístico* los pesos asignados son probabilidades de pertenencia a un grupo.
Lo normal es que este tipo de técnicas produzcan agrupamientos *exclusivos*, asignando cada elemento al
grupo que tenga el peso o probabilidad de pertenencia más alta.

Por último, se puede tratar todo el dataset o sólo una parte:

- **Clustering completo**: todos los elementos deben pertenecer, al menos, a un grupo.
- **Clustering parcial**: algunos elementos pueden no pertenecer a ningún grupo. La motivación de este
tipo de soluciones es que pueden existir elementos de nuestro conjunto de datos que no pertenezcan a grupos
bien definidos, se consideren ruido o sean atípicos que no proporcionan información (pero mucho cuidado con fallar
en esa interpretación) [@tan2019].

Por último, para generar los agrupamientos los algoritmos pueden fijarse en diferentes *propiedades*
o características de los datos. Si buscamos agrupamientos **bien separados**, se procurará agrupar elementos
cercanos entre sí y separados de los elementos en otros grupos. Los algoritmos **basados en prototipos**
asignan los elementos a un agrupamiento en función de su distancia al *prototipo* (elemento representativo)
de ese agrupamiento. Con frecuencia se denomian *clusters basados en centros*. Las estrategias **basadas
en grafos** agrupan elementos en función de su grado de conexión en una red. Los métodos **basados en densidad**
agrupan regiones con gran densidad de objetos contiguos los unos a los otros, separadas por otras regiones
menos densas a su alrededor. Se emplean usualmente con grupos de formas irregulares o entrecruzados, cuando
hay presencia de ruido y valores atípicos [@tan2019]. Por último, en los **métodos conceptuales** se agrupan
elementos que compartan propiedades comunes.

### Clustering no jerárquico

#### *k*-means

Es un algoritmo de *clustering* **particionado**, basado en **prototipos**. El número total de grupos a
identificar, denotado por $k$, debe ser *definido por el usuario*. Cada grupo está representado por un
**centroide**, que es un elemento *ficticio* (no un elemento real del dataset), que representa el punto
de referencia o representativo de todos los elementos incluidos en su grupo.

La @fig-kmeans-clust muestra gráficamente el proceso iterativo de definición de los agrupamientos hasta
llegar a una solución convergente (pero que puede no ser óptima). Se trata de un problema NP-hard, que
debe resolverse mediante heurísticos aproximados que no garantizan la mejor solución, aunque se pueden
tomar medidas para encontrar buenos agrupamientos.

![Proceso iterativo para definir los agrupamientos en el algoritmo *k*-means. Fuente: Wikipedia](img/K-means-convergence.gif){#fig-kmeans-clust width="70%"}

Si denotamos como $C_1, \dots, C_k$ los conjuntos que contienen los índices de las observaciones que
se asignan a cadad grupo, dichos conjuntos satisfacen dos propiedades:

1. $C_1 \cup C_2 \cup \dots \cup C_k = {1, \dots, n}$; es decir, cada observación pertenece al menos a
uno de los conjuntos.
2. $C_j \cap C_{j'} = \emptyset;\, \forall j \neq j'$, es decir, los clusters no tienen solape entre sí y
ninguna observación pertenece a más de un cluster.

El objetivo que persigue *k*-means es que la *variación intra-cluster* sea lo más pequeña posible. En
inglés, esa **within-cluster variation** para el grupo $C_j$ es una medida $W(C_j)$ de la cantidad de
diferencia que existe entre los elementos asignados a ese grupo. Por tanto, el objetivo es resolver
el problema expresado por la @eq-k-means-obj:

$$
min_{C_1,\dots,C_k} \left\{ \sum_{j=1}^k W(C_j) \right\}
$$ {#eq-k-means-obj}

En otras palabras, se pretende minimizar la suma en todos los grupos de la variación registrada dentro
de cada grupo. Si utilizamos la distancia Euclídea, entonces W(C_k) se convierte en la función conocida
como *Within-Cluster Sum of Squares* (WCSS).

::: {.callout-caution}
## Otras métricas de distancia en *k*-means

Debemos tener mucho cuidado con la utilzación del algoritmo *k*-means si queremos emplear otras funciones
métricas de distancia distintas de la distancia Euclídea (norma $L_2$). La mayoría de paquetes y bibliotecas
software **asumen por defecto** que se va a utilizar dicha distancia, ya que la función objetivo que se
plantea en el problema de optimización que debe resolver es, precisamente, minimizar el WCSS. Como acabamos
de ver arriba, este criterio *sólo es valido para la distancia Euclídea*. Existen algunas implementaciones
del algoritmo con otras funciones de distancia, pero no son para nada habituales.
:::

#### *k*-medoids

Es un algoritmo clásico de particionado que divide el conjunto de datos en $k$ grupos. Nuevamente, este
valor $k$ debe ser *definido por el usuario*. A diferencia del método anterior, este algoritmo identifica
un elemento *real* de los datos como el *prototipo* o representante de los elementos incluidos en su grupo.
Esto facilita mucho la interpretación de los resultados. El nombre del algoritmo fue popularizado por
L. Kaufman y P.J. Rousseeuw con su algoritmo PAM (*Partitioning Around Medoids*).

La @fig-kmedoids-clust muestra visualmente el proceso iterativo para encontrar los agrupamientos.

![Proceso iterativo para definir los agrupamientos en el algoritmo *k*-medoids. Fuente: Wikipedia](img/K-Medoids-Clustering.gif){#fig-kmedoids-clust width="70%"}

Como ocurría en el caso de *k*-means, se trata de un problema NP-hard para el que no se puede obtener
una solución exacta, por lo que se suelen emplear heurísticos para solucionarlo. En el @sec-part-clustering
veremos algunos ejemplos de variantes para solucionarlo implementadas en R.

#### DBSCAN

DBSCAN (*Density-Based Spatial Clustering of Applications with Noise*) es un algoritmo de agrupamiento 
basado en la densidad. A diferencia de métodos como *k*-means, que asumen que los clústeres 
quedan definidos por regiones de Voronoi basadas en la función de distancia (típicamente hiperesferas con distancia
Euclídea), DBSCAN define los clústeres como regiones continuas de alta densidad de puntos, separadas 
por regiones de baja densidad.

Su funcionamiento se rige por dos parámetros globales:

- $\varepsilon$: define el radio de vecindad alrededor de cada punto.
- $minPts$: mínimo número de puntos que deben caer dentro del radio de vecindad $\varepsilon$ para que esa
región se considere *densa*.

En base a estos parámetros, el algoritmo clasifica los elementos del dataset en tres posibles categorías:

1. Punto Núcleo (*Core Point*): un punto $p$ es un núcleo si su vecindad $\varepsilon$ contiene al menos $minPts$
elementos (incluyéndose a sí mismo).
2. Punto Frontera (*Border Point*): un punto $q$ es frontera si es alcanzable desde un punto núcleo 
(está dentro de su distancia $\varepsilon$), pero tiene menos de $minPts$ en su propia vecindad.
3. Ruido (*Noise/Outlier*): un punto es ruido si no es ni núcleo ni frontera.

A diferencia de los dos ejemplos anteriores, DBSCAN no requiere especificar $k$, sino que el algoritmo
puede determinar el número de agrupamientos según la densidad de los datos. Es especialmente propicio para
detectar grupos con formas geométricas "complejas" (no convexas), como el "swiss roll", arcos, anillos o
zig-zags. Además, su diseño está pensado para detectar y filtrar los elementos que se consideren "ruido".
Se utiliza con frecuencia en bases de datos espaciales. No obstante, también tiene limitaciones, ya que
puede tener dificultades en datasets con densidad variable en diferentes regiones o con datasets representados
en espacios de alta dimensionalidad.

### Clustering jerárquico

En el clustering jerárquico, los agrupamientos están anidados y pueden formar una estructura jerárquica, que
se suele representar gráficamente en forma de un árbol llamado **dendograma**, como el que se presenta en la
@fig-dendogram.

![Ejemplo de un dendograma que representa los agrupamientos encontrados tras aplicar un algoritmo de *clustering* jerárquico. Fuente: Figura 21.1 de [@boehmke2019].](img/dendrogram.png){#fig-dendogram width="90%"}

Debemos tener presente que, en función del nivel en el que "cortemos" el dendograma, obtendremos un número
diferente de grupos. Por ejemplo, la @fig-dendogram-cuts

![Tres dendogramas que muestran diferentes cortes a distinto nivel del árbol. Izq.: dendograma original (sin cortes); (centro) corte a nivel 9, que genera dos agrupamientos; (dcha.) corte a nivel 5, que define 3 agrupamientos. Fuente: Figura 12.11 de [@james2021].](img/dendogram-cuts.png){#fig-dendogram-cuts width="95%"}

#### AGNES

El método de *clustering* aglomerativo parte de un conjunto de $n$ grupos, uno por cada elemento de nuestro
dataset. En cada iteración, une los dos grupos más similares hasta que se obtenga un único grupo que
contiene a todos los datos.

Para ello, se necesita establecer un criterio de "enlace" para fusionar los grupos, que determine la distancia
entre dos agrupamientos. Se pueden usar varios criterios:

- *Agrupamiento de enlace completo*: minimiza la distancia máxima entre las parejas de observaciones de dos
agrupamientos.

- *Agrupamiento de enlace promedio*: minimizar la media de las distancias entre las observaciones pertenecientes
a dos clusteres (calculadas por pares).
- *Agrupamiento de enlace mínimo o simple*: minimizar las distancias entre las observaciones más cercanas
en dos clústers.
- *Método de Ward*: minimizar la suma de las diferencias cuadráticas dentro de los clusteres. De ese modo, se
minimiza la varianza total del conglomerado.
- *Enlace de centroides*: se calcula la distancia entre el centroide del grupo A y el centroide del grupo B.

### Medidas de evaluación en clustering

La mayoría de las técnicas de evaluación de *clusering* intentan cuantificar la **cohesión** de los
grupos (es decir, cómo de cercanos o similares son los elementos dentro del grupo), así como la
**separación entre grupos** (es decir, cómo de lejos están los clústeres entre sí). Muchas de estas
medidas se pueden aplicar a ambos tipos de algoritmos, tanto jerárquicos como no jerárquicos.

El paquete `factoextra` contiene varias funciones de utilidad para evaluación de resultados de *clustering*
e interpretación gráfica de dicha evaluación. En particular, la función `fviz_nbclust` permite
determinar y visualizar el número óptimo de grupos, en función de varias posibles métricas. A
continuación, presentamos varias métricas *intrínsecas*, que miden la calidad geométrica de una 
partición dada, independientemente de cómo se haya generado esa partición (ya sea de *k*-means 
o de un corte en un dendrograma).

Preparamos unos datos para los ejemplos, escalando primero los valores de las variables para todos
esten en rangos comparables

```{r}
#| label: eval-example-data
#| message: false

library(factoextra)
library(cluster)
df <- scale(USArrests)
```

#### La silueta (*silhouette*)

La **silueta** (en inglés *silhouette*) cuantifica lo bien que se ajusta cada elemento a su propio
cluster, si lo comparamos con el vecino. Al calcular la silueta promedio de todas las agrupaciones
de elemtnos, podemos determinar si la solución de *clustering* es buena en conjunto, siendo mejores
los valores más elevados. Si comparamos la silueta promedio obtenida para diferentes valores
de `k`, podemos ver de todos ellos el que obtiene el valor más elevado.

La métrica de la silueta es **aplicable a ambos tipos de algoritmos de clustering**, tanto jerárquicos
como no jerárquicos. Veamos un ejemplo con varias configuraciones del parámetro `k` usando *k*-means 
sobre el conjunto de datos `USArrests`.

```{r}
#| label: fig-example-silhouette
#| fig-cap: "Comparativa de los valores de silueta promedio para varias configuraciones del parámetro `k` en clustering con *k*-means."
fviz_nbclust(df, kmeans, method = "silhouette")
```

```{r}
#| label: fig-vis-silhouette
#| fig-cap: "Visualización de los valores de silueta para cada elemento agrupado. Los valores negativos (barras en sentido descendente) indican puntos mal agrupados."
#| warning: false

kmeans_4 <- kmeans(df, centers=4, nstart = 25)
silueta <- silhouette(kmeans_4$cluster, dist(df))
fviz_silhouette(silueta, palette = "jco", print.summary = FALSE)
```

En la sección @sec-pam-case-study veremos un ejemplo práctico con código para inspeccionar los valores
reportados por la silueta como mal clasificados. La línea roja en el gráfico marca el valor promedio de
silueta de todos el conjunto de agrupamientos.

#### *Elbow method*

Se trata de un método directo para evaluar cuál podría ser el número de grupos $k$ más adecuado para
el problema de agrupamiento a resolver. En el caso de algoritmos partitivos, este valor es un parámetro
de configuración que se fija al inicio del problema, por lo que podemos probar con varias opciones y comparar
los resultados para la función de coste que determina el resultado. Sólo se suele usar con este tipo
de algoritmos, ya que para los jerárquicos se prefieren otros métodos de evaluación.

En la @fig-example-elbow vemos un ejemplo comparando diferentes valores de la métrica *WCSS* para varias
configuraciones del parámetro `k` usando *k*-means sobre el conjunto de datos `USArrests`.

```{r}
#| label: fig-example-elbow
#| fig-cap: "Ejemplo de un gráfico que compara los resultados del WCSS para varios valores del parámetro de configuración `k` en *k*-means."

fviz_nbclust(df, kmeans, method = "wss") +
   geom_vline(xintercept = 4, linetype = 2)
```

#### La brecha (*gap*)

Esta métrica compara la WCSS observada con la esperada de un conjunto de datos generados sintéticamente
de forma aleatoria. En este caso, también se utiliza con algoritmos partitivos (y para distancia Euclídea).
El mejor valor de `k` es el que maximiza la métrica de *gap*.

```{r}
#| label: fig-example-gap
#| fig-cap: "Ejemplo de un gráfico comparativo de los valores de *gap* obtenidos para diferentes valores del parámetro de configuración `k` con el algoritmo *k*-means."

fviz_nbclust(df, kmeans, method = "gap") +
   geom_vline(xintercept = 4, linetype = 2)
```

## Reducción de la dimensionalidad

### Análisis de componentes principales (PCA)

El Análisis de Componentes Principales (PCA por sus siglas en inglés) se refiere al proceso por el cuál
computamos las **componentes principales** del espacio de representación de nuestros datos y su posterior
utilización para comprender mejor los patrones e información que contiene el *dataset*. Las *componentes
principales* son las direcciones en el espacio de descripción de los datos a lo largo de las cuales los
elementos del dataset tienen *alta variabilidad*.

Se trata de una técnica de aprendizaje máquina no supervisado, puesto que sólo involucra un conjunto de
variables descriptivas de los datos, $\{x_1, x_2,\dots, x_d \}$, y ninguna variable de salida [@james2021].
Además de para el análisis, se puede usar como herramienta de visualización (permitiendo representar los
elementos en un espacio bidimensional o, al menos, de menos dimensiones que el original), así como también
para imputación de datos [@james2021].

La idea intuitiva detrás del PCA es que, si bien los datos están originalmente representados en un espacio
con $d$ dimensiones, no todas ellas contribuyen de igual forma a la descripción de los datos. En otras
palabras, no todas ellas son *"interesantes"*, donde la medida de interés radica en cuánto varían las
observaciones a lo largo de cada dimensión. Cada una de las direcciones o *componentes* principales
halladas mediante el PCA es una combinación lineal de las $d$ características descriptivas, cada una de
ellas contribuyendo con un cierto peso (*loading*). Así, la primera componente principal viene dada
por la combinación lineal normalizada de las variables que tiene la variabilidad más alta:

$$
Z_1 = \phi_{11}X_1 + \phi_{21}X_2+\dots+\phi_{d1}X_d
$$ {#eq-pca-z1}

Que sea normalizada implica que $\sum_{j=1}^p \phi_{j1}^2 = 1$. Los pesos o *loadings* son, precisamente,
el conjunto de coeficientes $\phi_{11}, \phi_{21},\dots,\phi_{d1}$. Numéricamente, el cálculo de los pesos
se resuelve mediante técnicas de optimización [@james2021]. Después, se repite el proceso para obtener
la segunda componente principal, que será la que recoja la mayor variabilidad de todas las combinaciones
lineales que **estén incorreladas** con $Z_1$. Así, cada componente principal debe ser *ortogonal* a todas
las anteriores (esto es consecuencia de que todas estén incorreladas entre sí).

La @fig-pca2comp-usarrests muestra las dos primeras componentes principales calculadas sobre el dataset
`USArrests`. En la @sec-pca-usarrests veremos un ejemplo práctico, con codigo en R, de cómo calcular un
PCA sobre este conjunto de datos.

![Representación gráfica de las dos primeras componentes principales calculadas sobre el conjunto de datos `USArrests`. Fuente: Figura 12.1 de [@james2021].](img/PCA-2comp-USArrests.png){#fig-pca2comp-usarrests width="95%"}

### t-distributed Stochastic Neighbor Embedding (t-SNE)

El algoritmo t-SNE (*t-Distributed Stochastic Neighbor Embedding*) es una técnica de **reducción de 
dimensionalidad no lineal** muy popular, utilizada principalmente para la visualización de conjuntos de 
datos de alta dimensión en un espacio de representación en 2D o 3D. Su objetivo principal es preservar 
la estructura local de los datos.

Sus principales características son:

- Estrategia para preservar la estructura local de los datos: t-SNE se centra en preservar las distancias 
pequeñas (puntos cercanos en el espacio original) en el espacio de baja dimensión. Esto significa que los 
puntos que estaban cerca en el *dataset* original deben seguir estando cerca en la gráfica 2D.

- Distribuciones de probabilidad: el algoritmo convierte las distancias Euclídeas en el espacio de alta 
dimensión en probabilidades condicionales que representan la similitud.

  - En el espacio original (alta dimensión), usa una distribución Gaussiana para medir la similitud.

  - En el espacio de destino (baja dimensión), usa la distribución t-Student (con un grado de libertad, 
  de ahí la "t" en el nombre), que tiene colas más pesadas en sus extremos. Esto mitiga el problema del 
  *crowding* (agolpamiento de puntos).

- Técnica no lineal: a diferencia del PCA (que solo encuentra proyecciones lineales), t-SNE puede capturar 
relaciones complejas y no lineales en los datos.

- Uso de un parámetro de **perplejidad** (*perplexity*): Este es el hiperparámetro más crítico que hay que
configurar para que funcione el algoritmo. Se puede interpretar como una medida del número efectivo de 
vecinos que t-SNE considera al agrupar cada punto.

  - Controla el equilibrio entre el enfoque local y global.

  - Un valor de perplejidad incorrecto puede generar visualizaciones engañosas.

t-SNE es una técnica sobresaliente para generar visualizaciones donde los clústeres naturales de los 
datos se separan visualmente en la gráfica 2D. Como hemos visto, puede capturar relaciones no lineales
en los datos, superando a PCA en ese aspecto (está restringido a relaciones lineales). Además, al emplear
la distribución t-Student en el espacio de menor dimensionalidad, favorece que los agrupamientos estén
más separados entre sí y aclara el aspecto del gráfico. Por último, la interpretación visual que ofrece
es muy directa: si los puntos están cercanos entre sí son similares, si están lejos entre sí son más
diferentes.

Sin embargo, su aplicación trae consigo varias limitaciones:

- En conjuntos de datos muy grandes ($n > 10K$) requiere gran cantidad de memoria, ya que su complejidad
es $O(N^2)$.
- El resultado gráfico obtenido es muy sensible al valor del parámetro *perplexity*. Pequeñas alteraciones
en su valor se traducen en cambios importantes en el aspecto del gráfico.
- La distancia entre agrupamientos no es directamente interpretable. Es decir, si dos agrupamientos están
cercanos entre sí, significa que son similares, pero no sabes con certeza *cuánto de similares* son en el
espacio transformado.
- No proporciona un mapeo determinista. No se puede generar una visualización para un conjunto de datos y
posteriormente añadir nuevos puntos a la misma gráfica. Se debe calcular para todo el conjunto a la vez.

### Uniform Manifold Approximation and Projection (UMAP)

Se trata de una técnica reciente de reducción de la dimensionalidad, que puede usarse de modo
similar a t-SNE (para representaciones gráficas), pero también como herramienta no lineal de
reducción de las dimensiones en el espacio de representación.

- [Manual oficial de UMAP](https://umap-learn.readthedocs.io/en/latest/).

Los datos deben cumplir tres supuestos iniciales:

- Los datos se distribuyen de modo uniforme en un manifold de Riemann.
- La métrica de Riemann es constante localmente (o se puede aproximar a una constante).
- El manifold está localmente conectado.

El [paquete `umap`](https://cran.r-project.org/web/packages/umap/index.html) en R implementa este
algoritmo. En la [página de descripción en GitHub](https://github.com/tkonopka/umap) se encuentran
algunos ejemplos de visualizaciones con UMAP para el dataset MNIST de dígitos.

Siendo un algoritmo que incluye componentes estocásticas (igual que t-SNE), no se puede garantizar
que el resultado de la proyección obtenida sea reproducible. No obstante, se pueden
[fijar ciertos valores](https://umap-learn.readthedocs.io/en/latest/reproducibility.html) que permiten
obtener resultados reproducibles, a costa de perder eficiencia.

La documentación oficial también incluye varios ejemplos de cómo podemos usar
[UMAP como herramienta de *clustering*](https://umap-learn.readthedocs.io/en/latest/clustering.html).