# Clustering particionado {#sec-part-clustering}

## El algoritmo *k*-means

A la hora de implementar en la práctica el algoritmo *k*-means, debemos tener en cuenta varios 
aspectos importantes.

- Ubicación de partida de los centroides: el algoritmo es sensible a la ubicación inicial de los
`k` centroides para cada grupo. Una mala ubicación puede ofrecer un resultado nada óptimo o, incluso, provocar
que el algoritmo no converja en un número razonable de iteraciones. Para resolver este problema, se
ha propuesto el algoritmo *k-means++* [@arthur2007], que inicializa los centroides de forma que se asegure
que la solución obtenida es razonablemente óptima.
- Función de distancia Euclídea: Como ya se ha avisado, la mayoría de implementaciónes de *k*-means
en paquetes software y bibliotecas asumen que se va a emplear la distancia Euclídea para comparar los elementos,
estableciendo como objetivo minimizar la WCSS.
- Método para elegir el número de agrupamientos `k`: puesto que no se puede saber a priori el mejor
valor de `k`, se suele aconsejar emplear algún tipo de métrica de evaluación para ponderar la bondad de los
grupo generados, comparando los resultados para diversos valores de `k`.

### Implementaciones y mejoras en *k*-means

Otro aspecto poco conocido pero muy interesante es la existencia de un gran número de implementaciones para
resolver el algoritmo *k*-means, que han ido surgiendo para acelerar el cómputo o ahorrar muchas operaciones
[@wang2020efficiency]. La @fig-kmeans-implem muestra una línea temporal que describe algunas de las mejoras
más sobresalientes que se han ido desarrollando.

![Línea temporal que muestra las implementaciones y mejoras más destacadas que se han desarrollado para resolver el problema del agrupamiento con *k*-means. Fuente: [@wang2020efficiency].](img/k-means-timeline.png){#fig-kmeans-implem width="100%"}

Es conveniente, siempre que sea posible, informarnos bien acerca de la implementación específica que integra
el software que vamos a utilizar para resolver *k*-means, o si existen varias alternativas cuál de ellas es
la más propicia para nuestros intereses.

### Ejemplo práctico en R

Ejemplo tomado del Capítulo 12 de [@nwanganga2020].

**Dataset: `college.csv`**

Este conjunto de datos contiene información sobre una lista de facultades y universidades en EE.UU.
Los datos provienen del Departamento Estadounidense de Educación y se han filtrado y modificado
para este ejemplo [@nwanganga2020]. En total, contiene 1.270 observaciones.

Las características son:

- `id`: integer, id unívoco para cada observación.
- `name`: nombre de la institución.
- `city`: nombre de la ciudad en la que se ubica la institución.
- `state`: abreviatura estándar con dos caracteres del nombre del estado en el que se encuentra la
institución.
- `region`: una de las cuatro regiones en las que está ubicada la institución. Valores: `"Northeast"`,
`"Midwest"`, `"West"`, `"South"`.
- `highest_degree`: es el máximo nivel de titulación ofrecido por la institución. Valores: `"Associate"`,
`"Bachelor"`, `"Graduate"`, `"Nondegree"`.
- `control`: naturaleza de la institución. Valores: `"Public"`, `"Private"`.
- `gender`: género de los estudiantes en la institución. Valores: `"CoEd"`, `"Male"`, `"Female"`.
- `admission_rate`: porcentaje del total de estudiantes que lo solicitan que son admitidos por la
institución.
- `sat_avg`: puntuación promedio de los solicitantes en el test SAT (rango: `400` a `1600`).
- `undergrads`: número de estudiantes de pregrado en la institución.
- `tuition`: coste de matriculación anual en la institución, en USD.
- `faculty_salary-avg`: salario mensual promedio de los miembros del claustro, en dólares.
- `loan_default_rate`: porcentaje de estudiantes que, más tarde, no pueden hacer frente a los pagos
del préstamo de su matrícula.
- `median_debt`: mediana del montante de la deuda contraída por estudiantes graduados, en USD.
- `long`: coordenada de longitud del campus principal.
- `lat`: coordenada de latitud del campus principal.

**Objetivo**: segmentar las facultades en grupos de observaciones similares entre sí, mediante el
algoritmo *k*-means. Para acelerar la ejecución del ejemplo, se limitan los cálculos a las facultades
ubicadas en el estado de Maryland, aunque los mismos procedimientos pueden aplicarse a otros subconjuntos
de los datos.

```{r}
#| label: kmeans-data-pkgs
#| message: false

# Install the packages needed for this code file
# install.packages(c("stats","factoextra","gridExtra","cluster"))

library(tidyverse)
library(factoextra)
library(gridExtra)
library(cluster)

# Load and preview the colleges and universities dataset.
college <- read_csv("data/college.csv", col_types = "nccfffffnnnnnnnnn")

# Get a preview of the data.
glimpse(college)
```

Preparamos el dataset.

```{r}
#| label: kmeans-data-prep

# Let's limit our dataset to only colleges in the state of Maryland.
# We also convert the name of each college to the row labels.
maryland_college <- college %>%
  filter(state == "MD") %>%
  column_to_rownames(var = "name")

# Let's take a look at the summary stats for the dataset.
maryland_college %>%
  select(admission_rate, sat_avg) %>%
  summary()

# The summary statistics show a wide range of values for our features.
# In order to avoid features with large ranges from dominating our model,
# we need to normalize the features using the scale() function for z-score normalization.
maryland_college_scaled <- maryland_college %>%
  select(admission_rate, sat_avg) %>%
  scale()

# What do we have now?
maryland_college_scaled %>%
  summary()
```

Ejecutamos *k*-means

```{r}
#| label: kmeans-exec

# We are now ready to attempt to cluster the data.
# We set the value for k to 3 and choose to use 25 different initial configurations.
# The use of set.seed() ensures that every time we run our code, we get the same results.
set.seed(1234)
k_3 <- kmeans(maryland_college_scaled, centers=3, nstart = 25)
```

Evaluamos los resultados

```{r}
#| label: kmeans-eval

# Let's take a look at the size of the clusters...
k_3$size

# ...and the cluster centers.
k_3$centers
```

```{r}
#| label: fig-kmeans-eval

# We can also visualize the clusters to get additional insight.
fviz_cluster(k_3,
             data = maryland_college_scaled,
             repel = TRUE,
             ggtheme = theme_minimal()) + theme(text = element_text(size = 14))
```

```{r}
#| label: kmeans-eval-ii

# To further evaluate our results, we need to look at how attributes vary by cluster.
maryland_college %>%
  mutate(cluster = k_3$cluster) %>%
  select(cluster,
         undergrads,
         tuition,
         faculty_salary_avg,
         loan_default_rate,
         median_debt) %>%
  group_by(cluster) %>%
  summarise_all("mean")
```

```{r}
#| label: fig-kmeans-optim
#| fig-cap: ""

# Let's see how varying the number of clusters affects the results.
k_4 <- kmeans(maryland_college_scaled, centers = 4, nstart = 25)
k_5 <- kmeans(maryland_college_scaled, centers = 5, nstart = 25)
k_6 <- kmeans(maryland_college_scaled, centers = 6, nstart = 25)

# Plot and compare the results.
p1 <- fviz_cluster(k_3, geom = "point", data = maryland_college_scaled) + ggtitle("k = 3")
p2 <- fviz_cluster(k_4, geom = "point", data = maryland_college_scaled) + ggtitle("k = 4")
p3 <- fviz_cluster(k_5, geom = "point", data = maryland_college_scaled) + ggtitle("k = 5")
p4 <- fviz_cluster(k_6, geom = "point", data = maryland_college_scaled) + ggtitle("k = 6")

# Here, we make use of the grid.arrange() function in the gridExtra package to display several plots at the same time.
library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

::: {.callout-tip}
## Pregunta: optimizando el número de centroides

A partir de los resultados del gráfico anterior, ¿puedes decir cuál es el número de centroides
óptimo para el agrupamiento de estos datos?
:::

```{r}
#| label: fig-kmeans-select-k
#| fig-cap: ""

# Now let's try to choose an ideal value for k based on the elbow method.
# Note: The geom_point() function is used to create additional circles on the plot.
fviz_nbclust(maryland_college_scaled, kmeans, method = "wss") +
  geom_point(
    shape = 1,
    x = 4,
    y = 7.3,
    colour = "red",
    size = 8,
    stroke = 1.5
  ) + 
  geom_point(
    shape = 1,
    x = 7,
    y = 2.3,
    colour = "red",
    size = 8,
    stroke = 1.5
  )
```

```{r}
#| label: fig-kmeans-silhouette
#| fig-cap: ""

# What about based on the silhouette method?
fviz_nbclust(maryland_college_scaled, kmeans, method = "silhouette") +
  geom_point(
    shape = 1,
    x = 4,
    y = 0.393,
    colour = "red",
    size = 8,
    stroke = 1.5
  ) +
  geom_point(
    shape = 1,
    x = 7,
    y = 0.375,
    colour = "red",
    size = 8,
    stroke = 1.5
  )
```

```{r}
#| label: fig-kmeans-gap
#| fig-cap: ""

# Now, let's see what the gap statistic tells us.
fviz_nbclust(maryland_college_scaled, kmeans, method = "gap_stat") +
  geom_point(
    shape = 1,
    x = 1,
    y = 0.218,
    colour = "red",
    size = 8,
    stroke = 1.5
  ) +
  geom_point(
    shape = 1,
    x = 7,
    y = 0.2,
    colour = "red",
    size = 8,
    stroke = 1.5
  )
```

```{r}
#| label: fig-kmeans-gap-alt
#| fig-cap: ""

# Alternatively, we can use fviz_gap_stat() function to generate the similar plot for gap statistics.
# Note: The two plot may not be exactly alike because fviz_nbclust() function does not specify the nstart argument. 
set.seed(1234)
gap_stat <- clusGap(maryland_college_scaled, FUN = kmeans, nstart = 25, K.max = 10, B = 10)
fviz_gap_stat(gap_stat)
```

::: {.callout-tip}
## Otra forma de calcular *k*

A partir de los resultados del gráfico anterior, ¿puedes decir cuál es el número de centroides
óptimo para el agrupamiento de estos datos?
:::

```{r}
#| label: fig-kmeans-koptim
#| fig-cap: ""

k_4 <- kmeans(maryland_college_scaled, centers = 4, nstart = 25)

fviz_cluster(
  k_4,
  data = maryland_college_scaled,
  main = "Maryland Colleges Segmented by SAT Scores and Admission Rates",
  repel = TRUE,
  ggtheme = theme_minimal()
) +
  theme(text = element_text(size = 14))
```

::: {.callout-tip}
## Ejercicios adicionales

1. Repite los cálculos anteriores para las escuelas ubicadas en el estado de Indinana. Céntrate en
las variables `faculty_salary-avg` y `tuition`. Elige `k=3` para visualizar los grupos identificados.
2. Emplea las técnicas descritas arriba para elegir el número óptimo de clústeres en el problema del
apartado 1). Justifica tu respuesta.
3. Genera diagramas de agrupamiento para los dos posibles valores de `k` que has seleccionado en el
apartado 2) anterior. ¿Cuál piensas que es el valor óptimo y por qué?
:::

## PAM: el algoritmo *k*-medoids

El algoritmo *k*-medoids tiene algunas importantes ventajas respecto de *k*-means:

- Los prototipos de cada grupo no son *centroides* (puntos ficticios o sintéticos) sino *medoides,
es decir, elementos *reales* del dataset. Esto facilita mucho la interpretación de los resultados,
puesto que se puede analizar las características de cada elemento prototipo para concluir cuáles
son los principales rasgos de los elementos de ese grupo.
- Mientras que en muchas implementaciones de *k*-means, se asume que vamos a emplear la función de
distancia Euclídea y que se busca minimizar la WCSS para resolver el problema, en las implementaciones
de *k*-medoids se pueden utilizar **funciones de distancia arbitrarias** para comparar y agrupar
los elementos.

También existen implementaciones con rendimiento mejorado de este algoritmo. Por ejemplo, el paquete
[`fastkmedoids` en R](https://cran.r-project.org/web/packages/fastkmedoids/refman/fastkmedoids.html)
proporciona *wrappers* para las implementaciones en C++ de varios algoritmos más rápidos para 
resolver el problema PAM (FastPAM, FastCLARA Y FastCLARANS) [@schubert2019fastPAM].

### Ejemplo de implementación en R



