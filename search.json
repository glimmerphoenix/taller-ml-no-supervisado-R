[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aprendizaje máquina no supervisado con R",
    "section": "",
    "text": "Prefacio\nEn este taller exploramos aspectos prácticos del diseño, implementación y evaluación de modelos de aprendizaje automático no supervisado. Estos modelos pueden aplicarse en diversos problemas en los que no contamos con una serie de ejemplos de aprendizaje previamente resueltos. Por tanto, el objetivo principal de estos algoritmos será descubrir patrones interesantes o ayudarnos a sintentizar la información contenida en conjuntos de datos complejos (en muchos casos, en espacios de representación de alta dimensionalidad y con alta dispersión de valores).\nEste es un taller práctico, que presenta ejemplos reales y comandos para entrenar, evaluar y aplicar modelos de aprendizaje automático no supervisado con R. Además, junto a la explicación de los conceptos clave para entender este proceso también se ofrecen recomendaciones sobre buenas prácticas metodológicas para comparar y elegir el algoritmo o modelo más adecuado para una aplicación específica, así como herramientas para evaluar e interpretar mejor los resultados obtenidos.\nLos apuntes para este taller práctico se han realizado con Quarto, una herramienta para creación de documentación científica y programación literaria compatible con R y otros lenguajes de programación científica.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#requisitos-previos",
    "href": "index.html#requisitos-previos",
    "title": "Aprendizaje máquina no supervisado con R",
    "section": "Requisitos previos",
    "text": "Requisitos previos\nPara poder realizar los ejemplos inlcuidos en este taller necesitas tener instalado R y una IDE de desarrollo para este lenguaje. Se recomienda instalar RStudio o MS Visual Code como entorno de programación.\n\nInstalación de R.\nInstalación de RStudio.\n\nAdicionalmente, es necesario instalar una serie de paquetes R antes de ejecutar los ejemplos, para que todas las dependencias estén disponibles en nuestro sistema. Consulta el Apéndice ?sec-pkg-requirements para comprobar el listado de paquetes R necesarios.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 Tipos de aprendizaje automático\nExisten muchas definiciones complementarias sobre la Inteligencia Artificial (IA), cada una reflejo de un modo diferente de aproximarse al complejo problema de crear máquinas que puedan imitar la inteligencia humana (Russell & Norvig, 2009). Por ejemplo, en el artículo de Wikipedia sobre IA se define de la siguiente forma enlace:\nEn 1950, Alan Turing propuso el llamado Test de Turing para proporcionar una definción operativa satisfactoria sobre “inteligencia”. En teoría, un computador supera el test si un interrogador humano, después de plantear varias preguntas, no es capaz de discernir si las respuestas escritas provienen de una persona o de un computador. Para poder superar el test, un computador necesitaría incluir muchas habilidades. Aunque el test original no lo contemplaba, el llamado Test de Turing total incorpora una señal de vídeo para que el interrogador pueda evaluar las habilidades de percepción y reacción físicas del sujeto. Entre otros aspectos, esto implica contar con habilidades como:\nPor tanto, vemos que el aprendizaje automático (machine learning o ML en inglés) es una rama de la IA que tiene por objetivo el desarrollo de técnicas y métodos para que las computadoras aprendan. Un agente inteligente aprende cuando es capaz de mejorar su rendimiento a partir de la experiencia y de la información extraída de datos.\nExiste una taxonomía ampliamente aceptada para clasificar las diferentes aproximaciones para resolver el problema del aprendizaje automático (Russell & Norvig, 2009):",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#tipos-de-aprendizaje-automático",
    "href": "01-introduccion.html#tipos-de-aprendizaje-automático",
    "title": "1  Introducción",
    "section": "",
    "text": "Aprendizaje supervisado: el agente observa algunos ejemplos de parejas de valores entrada-salida y aprende una función que mapea nuevas entradas a nuevos valores de salida.\nAprendizaje no supervisado: el agente debe descubrir patrones o similitudes entre los valores de entrada aunque no se le haya facilitado ningún tipo de información adicional.\nAprendizaje semisupervisado: combina los dos anteriores, puesto que el agente recibe sólo unos pocos ejemplos y, a partir de ellos, tiene que hacer lo posible por descubrir patrones en una colección de ejemplos no etiquetados o explicados.\nAprendizaje por refuerzo: el método de aprendizaje implica que el agente reciba premios o castigos. Por ejemplo, si un agente aprende a jugar al ajedrez y recibe dos puntos por haber ganado una partida, deduce que lo ha hecho bien. Por contra, si un agente de conducción registra un aviso de choque recibe una penalización para indicarle que lo ha hecho mal.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#tareas-de-aprendizaje-automático-no-supervisado",
    "href": "01-introduccion.html#tareas-de-aprendizaje-automático-no-supervisado",
    "title": "1  Introducción",
    "section": "1.2 Tareas de aprendizaje automático no supervisado",
    "text": "1.2 Tareas de aprendizaje automático no supervisado\n\n1.2.1 Clustering\nEl agrupamiento o clustering es una tarea que tiene por objetivo agrupar los datos en función del grado de similaridad (semejanza) que existe entre los elementos de un dataset. Por lo tanto, lo primero que debemos hacer es definir qué es una función de similaridad y su relación con la función de distancia.\nConsideremos un conjunto \\(\\mathcal{X}\\) de \\(n\\) elementos, descritos por una serie de \\(d\\) variables, \\(\\{x_1, x_2,\\dots, x_d \\}\\). Suponiendo que todas esas variables se pueden expresar mediante valores numéricos reales, cada elemento del conjunto \\(\\mathcal{X}\\) quedaría pues representado en un espacio \\(\\mathbb{R}^d\\). Una función de similaridad es una aplicación \\(s: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}\\) que cumple estas propiedades para tres elementos \\(A, B\\) y \\(C\\) de \\(\\mathcal{X}\\) (Xu & II, 2009):\n\nPositividad: \\(0 \\leq s(A,B) \\leq 1,\\; \\forall A,B\\).\nSimetría: \\(s(A,B) = s(B,A)\\).\n\nSi además cumple las siguientes propiedades:\n\n\\(s(A,B)S(B,C) \\leq [S(A,B)+S(B,C)]S(A,C)\\);\nReflexividad: \\(S(A,B)=1 \\Leftrightarrow A=B\\),\n\nentonces se denomina métrica de similaridad.\nPor tanto, la similaridad cuantifica cómo de parecidos son dos elementos de un conjunto de datos, según los valores de las variables que los describen. Existen definiciones análogas para determinar las propiedades de una función de similaridad con otros tipos de variables (por ejemplo, categóricas u ordinales). La noción opuesta a la expresada por la similaridad es la disimilaridad, que cuantifica lo alejados o diferentes que son dos elementos de \\(\\mathcal{X}\\) entre sí. Es sencillo obtener una función de disimilaridad a partir de la función de similaridad, puesto que está normalizada.\nSi denotamos la función de disimilaridad como \\(\\delta(A,B)\\) esta se puede obtener como:\n\\[\n\\delta(A,B) = 1 - S(A,B).\n\\]\nUna función de distancia, que podemos denotar como \\(DdA,B)\\) mide también lo distintos que son entre sí una pareja de elementos \\((A,B)\\) de \\(\\mathcal{X}\\). En el caso de las distancias métricas, éstas cumplen cuatro propiedades de los espacios métricos (Skopal, 2007):\n\nReflexividad: \\(d(A,B) = 0 \\Leftrightarrow A = B\\).\nNo negatividad: \\(d(A,B) &gt; 0 \\Leftrightarrow A \\neq B\\).\nSimetría: \\(d(A,B) = D(B,A)\\).\nDesigualdad triangular: \\(d(A,B) + d(B,C) \\geq d(A,C)\\).\n\nEn lo sucesivo, vamos a circunscribir nuestra descripción a las distancias métricas, puesto que se utilizan en un gran número de problemas prácticos y simplificarán muchas consideraciones que haremos respecto de los algoritmos de clustering que vamos a tratar. Por ejemplo, una familia de distancias ampliamente conocida es la representada por la distancia Minkowski de orden \\(p &gt; 0\\), que se define como:\n\\[\nd_{Mink}(A,B) = \\left( \\sum_{j=1}^d |x_{Aj}-x_{Bj}|^p \\right)^{1/p},\n\\tag{1.1}\\]\nsiendo \\(\\|A\\|_p = \\left( \\sum_{j=1}^d |x_{Aj}|^p \\right)^{1/p}\\) la norma \\(p* (denotada muchas veces como\\)L_p$) del vector A. Así, para \\(p=1\\) tenemos la distancia Manhattan (\\(L_1\\)), mientras que para \\(p=2\\) tenemos la popular distancia Euclídea (\\(L_2\\)).\nSe denomina matriz de distancias a una matriz \\(D=(d_{E_{i}E_{j}});\\, i,j=1,\\dots,n\\) que contiene todas las distancias entre cada par de elementos \\((E_i,E_j)\\) de \\(\\mathcal{X}\\). Siempre que la función de distancia utilizada cumpla la propiedad de simetría, esta matriz será triangular.\n\n\n\n\n\n\nDistancias no métricas\n\n\n\nAunque es muy habitual utilizar distancias métricas en los problemas de clustering y de búsqueda por similaridad, también existen muchas funciones de distancia de gran utilidad en muchos problemas que no cumplen las cuatro propiedades enunciadas (Skopal, 2007). Por ejemplo, las semimétricas son distancias que no cumplen la cuarta propiedad de desigualdad triangular. Por otro lado, las pseudométricas no satisfacen la reflexividad. Por último, las cuasimétricas no satisfacen la simetría.\n\n\n\n1.2.1.1 Algunas funciones de distancia\nLa Tabla 1.1 resume información básica sobre algunas funciones de distancia comúnmente empleadas.\n\n\n\nTabla 1.1: Algunas funciones de distancia muy frecuentes.\n\n\n\n\n\n\n\n\n\n\nFunción distancia\nEs métrica\nAplicaciones\n\n\n\n\nManhattan (city block)\nSí\nMultiples: análisis de imagen, tiempos de viaje\n\n\nEuclídea\nSí\nUna de las funciones más utilizadas\n\n\nMinkowski\nSí, \\(p\\geq1\\)\nGeneralización de funciones de disancia básicas\n\n\nMahalanobis\nSí\nComparación entre distribuciones de valores\n\n\nSimilaridad del coseno\nNo(*)\nAgrupamiento y comparación de documentos\n\n\nJaccard\nSí\nDatos binarios en ecología, genómica, etc.\n\n\n\n\n\n\n(*): Se puede efectuar una trasnformación a una distancia “comparable” que sí cumple las propiedades de métrica.\n\n\n1.2.1.2 Tipos de clustering\nEn general, las estrategias de aprendizaje no supervisado para clustering se pueden clasificar en dos grandes grupos, cuyo enfoque conceptual se ilustra en la Figura 1.1.\n\n\n\n\n\n\nFigura 1.1: Ilustración de los dos posibles tipos de algoritmos de clustering: (izq.) jerárquico; (dcha.) no jerárquico o partitivo. Fuente: Figura 12.2 de (Nwanganga & Chapple, 2020).\n\n\n\n\nClustering jerárquico: en este enfoque, el algoritmo genera grupos de elementos que están contenidos unos dentro de los otros. Por lo tanto, los límites de un agrupamiento pueden estar dentro de los límites de otra agrupación de mayor nivel, creando una jerarquía de relaciones padre-hijo entre los clústeres. Esta relación se suele representar gráficamente por medio de un diagrama de árbol que se denomina dendograma.\nClustering no jerárquico (partitional): la estrategia de este tipo de algoritmos consiste en particionar el espacio de representación de un cierto número de grupos, siendo uno de los objetivos conseguir que no exista solape entre los grupos identificados y, por tanto, cada grupo es independiente del resto.\n\nPor ejemplo, la Figura 1.2 muestra las regiones determinadas por un algoritmo no jerárquico como k-means cuando divide el espacio de representación (asumimos que bidimensional) en varias regiones, asignando un punto representativo o centroide a cada región.\n\n\n\n\n\n\nFigura 1.2: Ilustración de un diagrama o tesela de Voronoi, usando la distancia Euclídea. Fuente: Wikipedia.\n\n\n\nPor último, tenemos otra taxonomía de algoritmos de clustering en función de si los agrupamientos pueden o no tener solapes entre sí, como muestra la Figura 1.3.\n\n\n\n\n\n\nFigura 1.3: Ilustración de dos posibles tipos de algoritmos de clustering: (izq.) con solape entre grupos; (dcha.) sin solape entre grupos. Fuente: Figura 12.3 de (Nwanganga & Chapple, 2020).\n\n\n\n\n\n\n1.2.2 Reducción de dimensionalidad\nOtra tarea muy habitual en el aprendizaje automático no supervisado es la de expresar un conjunto de datos de manera más eficiente, reduciendo el número de variables que se emplean para describir sus elementos. Para ello, se busca identificar un conjunto más pequeño de variables, que se suelen denominar componentes principales o variables latentes. Estas nuevas variables deben ser capaces de capturar la misma cantidad de información sobre los elementos de nuestro conjunto de datos pero en un espacio de representación con muchas menos dimensiones.\nConsideremos el dataset MNIST, que contiene 60.000 imágenes de entrenamiento y 10.000 imágenes de prueba, cada una de ellas con información sobre imágenes de dígitos manuscritos sobre una malla de 28x28 píxeles. Por tanto, cada elemento de este dataset está representado por 784 variables (una por cada pixel). La Figura 1.4 muestra algunos ejemplos de estas cifras manuscritas incluidas en el dataset.\n\n\n\n\n\n\nFigura 1.4: Ejemplos de algunos de los dígitos manuscritos incluidos en el dataset MNIST.\n\n\n\nSería muy complicado conseguir la representación gráfica de todos estos elementos para detectar las imágenes de dígitos que son parecidas entre sí (por ejemplo, un grupo con los números 4, otro con los números 8, etc.). Las técnicas de reducción de dimensionalidad nos permiten proyectar la representación original de estos datos en un espacio \\(\\mathbb{R}^784\\) sobre un espacio \\(\\mathbb{R}^2\\), que sí podemos representar gráficamente.\nPor ejemplo, la Figura 1.5 muestra una representación gráfica en 2D de una muestra aleatoria de elementos de MNIST, en la que previamente se han reducido las 784 dimensiones originales a 2 mediante el algoritmo t-SNE (Maaten & Hinton, 2008).\n\n\n\n\n\n\nFigura 1.5: Representación gráfica de la proyección 2D de una muestra de elementos del conjunto de datos MNIST, obtenida mediante el algoritmo t-SNE (Maaten & Hinton, 2008). Fuente: elaboración propia.\n\n\n\nVeremos que una limitación importante de estos algoritmos consiste en que las variables que obtenemos tras la reducción de las dimensiones originales no son, en muchos casos, fácilmente interpretables.\n\n\n\n\nMaaten, L. van der, & Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605.\n\n\nNwanganga, F., & Chapple, M. (2020). Practical Machine Learning in R (1.ª ed.). John Wiley & Sons, Inc. https://www.wiley.com/en-us/Practical+Machine+Learning+in+R-p-9781119591535\n\n\nRussell, S., & Norvig, P. (2009). Artificial Intelligence: A Modern Approach (3.ª ed.). Prentice Hall Press.\n\n\nSkopal, T. (2007). Unified framework for fast exact and approximate search in dissimilarity spaces. ACM Transactions on Database Systems, 32(4), 29. https://doi.org/10.1145/1292609.1292619\n\n\nXu, R., & II, D. C. W. (2009). Clustering. Wiley-IEEE Press. https://ieeexplore.ieee.org/book/5236612",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-ml-no-supervisado.html",
    "href": "02-ml-no-supervisado.html",
    "title": "2  Aprendizaje máquina no supervisado",
    "section": "",
    "text": "2.1 Clustering\nEl análisis cluster (agrupamiento) identifica grupos de objetos basándose únicamente en la información que proporcionan los datos que describen esos objetos y sus relaciones entre sí. El objetivo es que los objetos asignados al mismo grupo se similares o estén relacionados entre sí y, a su vez, sean distintos (o no estén relacionados) con los elementos en otros grupos. En consecuencia, cuanto mayor sea la homogeneidad dentro de un mismo grupo y mayor sea la diferencia entre grupos, mejor o más distintivo será el clustering (Tan et al., 2019).\nSin embargo, identificar los posibles agrupamientos en un conjunto de datos no es tan sencillo como puede parecer a primera vista. La Figura 2.1 muestra un conjunto de tan sólo 20 elementos en el que podemos formar, al menos, tres posibles agrupamientos de 2, 4 y 6 grupos, respectivamente. ¿Cuál es la mejor opción? En muchos casos, no sólo depende de las métricas de evaluación que utilicemos, sino también de las descripciones disponibles sobre los datos y del conocimiento del dominio de aplicación del problema.\nExisten diferentes tipos de algoritmos de agrupamiento o clustering, en función de las propiedades de los grupos que obtenemos como resultado del proceso.\nEn función de si existe o no solape entre los grupos tenemos:\nPor último, se puede tratar todo el dataset o sólo una parte:\nPor último, para generar los agrupamientos los algoritmos pueden fijarse en diferentes propiedades o características de los datos. Si buscamos agrupamientos bien separados, se procurará agrupar elementos cercanos entre sí y separados de los elementos en otros grupos. Los algoritmos basados en prototipos asignan los elementos a un agrupamiento en función de su distancia al prototipo (elemento representativo) de ese agrupamiento. Con frecuencia se denomian clusters basados en centros. Las estrategias basadas en grafos agrupan elementos en función de su grado de conexión en una red. Los métodos basados en densidad agrupan regiones con gran densidad de objetos contiguos los unos a los otros, separadas por otras regiones menos densas a su alrededor. Se emplean usualmente con grupos de formas irregulares o entrecruzados, cuando hay presencia de ruido y valores atípicos (Tan et al., 2019). Por último, en los métodos conceptuales se agrupan elementos que compartan propiedades comunes.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aprendizaje máquina no supervisado</span>"
    ]
  },
  {
    "objectID": "02-ml-no-supervisado.html#clustering",
    "href": "02-ml-no-supervisado.html#clustering",
    "title": "2  Aprendizaje máquina no supervisado",
    "section": "",
    "text": "Figura 2.1: Un conjunto de 20 elmentos y cuatro posibles agrupamientos, con distinto número de grupos en cada caso. Basado en la Fig. 5.1 de (Tan et al., 2019).\n\n\n\n\n\nClústering jerárquico (o anidado): en este caso el resultado son agrupamientos anidados, unos dentro de los otros, que conforman una jerarquía de grupos. Según la estrategia empleada para crear los grupos, pueden ser:\n\nAglomerativos: se parte de un primer grupo de datos, y se van uniendo progresivamente los siguientes grupos de datos más similares, hasta que todos los datos quedan en un único grupo a alto nivel. Normalmente, este algoritmo recibe el nombre de AGNES (AGlomerative NESting) (Boehmke & Greenwell, 2019). Es mejor detectando grupos de pequeño tamaño, con pocos elementos.\nDivisivos: se comienza considerando a todos los datos dentro de un mismo grupo, y progresivamente se van dividiendo en grupos anidados, de modo que los elementos más cercanos queden en el mismo grupo. Este algoritmo se denomina con frecuencia DIANA (DIvise ANAlysis). Es más adecuado para detectar grupos de gran tamaño, con muchos elementos.\n\nClústering partitivo (no jerárquico): propone una división sin solape entre los grupos de elementos, de forma que cada grupo contenga elementos parecidos entre sí. Lo normal es que sigan un procedimiento iterativo para resolver el problema, cuya solución exacta no se puede hallar computacionalmente.\n\n\n\nClustering sin solape: es el tipo de clustering propuesto por los algoritmos partitivos. También hay que considerar el clustering jerárquico como sin solape, puesto que la relación entre agrupamientos anidados se produce a distintos niveles (no es lo mismo un solape entre dos grupos al mismo nivel que el hecho de que un grupo esté contenido en otro de mayor nivel jerárquico).\nClustering con solape (no exclusivo): permite que un elemento pueda pertenecer a más de un grupo a la vez. Es decir, pueden existir solapes entre las regiones definidas para cada grupo.\nClustering difuso (fuzzy clustering): todo objeto pertenece a todos los agrupamientos con un cierto grado (peso) de pertenencia entre 0 (no pertenece en absoluto) y 1 (pertenece con toda seguridad). Usualmente, se impone la condición adicional de que la suma de los pesos asignados a un elemento sea 1. En el clustering probabilístico los pesos asignados son probabilidades de pertenencia a un grupo. Lo normal es que este tipo de técnicas produzcan agrupamientos exclusivos, asignando cada elemento al grupo que tenga el peso o probabilidad de pertenencia más alta.\n\n\n\nClustering completo: todos los elementos deben pertenecer, al menos, a un grupo.\nClustering parcial: algunos elementos pueden no pertenecer a ningún grupo. La motivación de este tipo de soluciones es que pueden existir elementos de nuestro conjunto de datos que no pertenezcan a grupos bien definidos, se consideren ruido o sean atípicos que no proporcionan información (pero mucho cuidado con fallar en esa interpretación) (Tan et al., 2019).\n\n\n\n2.1.1 Clustering no jerárquico\n\n2.1.1.1 k-means\nEs un algoritmo de clustering particionado, basado en prototipos. El número total de grupos a identificar, denotado por \\(k\\), debe ser definido por el usuario. Cada grupo está representado por un centroide, que es un elemento ficticio (no un elemento real del dataset), que representa el punto de referencia o representativo de todos los elementos incluidos en su grupo.\nLa Figura 2.2 muestra gráficamente el proceso iterativo de definición de los agrupamientos hasta llegar a una solución convergente (pero que puede no ser óptima). Se trata de un problema NP-hard, que debe resolverse mediante heurísticos aproximados que no garantizan la mejor solución, aunque se pueden tomar medidas para encontrar buenos agrupamientos.\n\n\n\n\n\n\nFigura 2.2: Proceso iterativo para definir los agrupamientos en el algoritmo k-means. Fuente: Wikipedia\n\n\n\nSi denotamos como \\(C_1, \\dots, C_k\\) los conjuntos que contienen los índices de las observaciones que se asignan a cadad grupo, dichos conjuntos satisfacen dos propiedades:\n\n\\(C_1 \\cup C_2 \\cup \\dots \\cup C_k = {1, \\dots, n}\\); es decir, cada observación pertenece al menos a uno de los conjuntos.\n\\(C_j \\cap C_{j'} = \\emptyset;\\, \\forall j \\neq j'\\), es decir, los clusters no tienen solape entre sí y ninguna observación pertenece a más de un cluster.\n\nEl objetivo que persigue k-means es que la variación intra-cluster sea lo más pequeña posible. En inglés, esa within-cluster variation para el grupo \\(C_j\\) es una medida \\(W(C_j)\\) de la cantidad de diferencia que existe entre los elementos asignados a ese grupo. Por tanto, el objetivo es resolver el problema expresado por la Ecuación 2.1:\n\\[\nmin_{C_1,\\dots,C_k} \\left\\{ \\sum_{j=1}^k W(C_j) \\right\\}\n\\tag{2.1}\\]\nEn otras palabras, se pretende minimizar la suma en todos los grupos de la variación registrada dentro de cada grupo. Si utilizamos la distancia Euclídea, entonces W(C_k) se convierte en la función conocida como Within-Cluster Sum of Squares (WCSS).\n\n\n\n\n\n\nOtras métricas de distancia en k-means\n\n\n\nDebemos tener mucho cuidado con la utilzación del algoritmo k-means si queremos emplear otras funciones métricas de distancia distintas de la distancia Euclídea (norma \\(L_2\\)). La mayoría de paquetes y bibliotecas software asumen por defecto que se va a utilizar dicha distancia, ya que la función objetivo que se plantea en el problema de optimización que debe resolver es, precisamente, minimizar el WCSS. Como acabamos de ver arriba, este criterio *sólo es valido para la distancia Euclídea**. Existen algunas implementaciones del algoritmo con otras funciones de distancia, pero no son para nada habituales.\n\n\n\n\n2.1.1.2 k-medoids\nEs un algoritmo clásico de particionado que divide el conjunto de datos en \\(k\\) grupos. Nuevamente, este valor \\(k\\) debe ser definido por el usuario. A diferencia del método anterior, este algoritmo identifica un elemento real de los datos como el prototipo o representante de los elementos incluidos en su grupo. Esto facilita mucho la interpretación de los resultados. El nombre del algoritmo fue popularizado por L. Kaufman y P.J. Rousseeuw con su algoritmo PAM (Partitioning Around Medoids).\nLa Figura 2.3 muestra visualmente el proceso iterativo para encontrar los agrupamientos.\n\n\n\n\n\n\nFigura 2.3: Proceso iterativo para definir los agrupamientos en el algoritmo k-medoids. Fuente: Wikipedia\n\n\n\nComo ocurría en el caso de k-means, se trata de un problema NP-hard para el que no se puede obtener una solución exacta, por lo que se suelen emplear heurísticos para solucionarlo. En el Capítulo 3 veremos algunos ejemplos de variantes para solucionarlo implementadas en R.\n\n\n2.1.1.3 DBSCAN\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de agrupamiento basado en la densidad. A diferencia de métodos como k-means, que asumen que los clústeres quedan definidos por regiones de Voronoi basadas en la función de distancia (típicamente hiperesferas con distancia Euclídea), DBSCAN define los clústeres como regiones continuas de alta densidad de puntos, separadas por regiones de baja densidad.\nSu funcionamiento se rige por dos parámetros globales:\n\n\\(\\varepsilon\\): define el radio de vecindad alrededor de cada punto.\n\\(minPts\\): mínimo número de puntos que deben caer dentro del radio de vecindad \\(\\varepsilon\\) para que esa región se considere densa.\n\nEn base a estos parámetros, el algoritmo clasifica los elementos del dataset en tres posibles categorías:\n\nPunto Núcleo (Core Point): un punto \\(p\\) es un núcleo si su vecindad \\(\\varepsilon\\) contiene al menos \\(minPts\\) elementos (incluyéndose a sí mismo).\nPunto Frontera (Border Point): un punto \\(q\\) es frontera si es alcanzable desde un punto núcleo (está dentro de su distancia \\(\\varepsilon\\)), pero tiene menos de \\(minPts\\) en su propia vecindad.\nRuido (Noise/Outlier): un punto es ruido si no es ni núcleo ni frontera.\n\nA diferencia de los dos ejemplos anteriores, DBSCAN no requiere especificar \\(k\\), sino que el algoritmo puede determinar el número de agrupamientos según la densidad de los datos. Es especialmente propicio para detectar grupos con formas geométricas “complejas” (no convexas), como el “swiss roll”, arcos, anillos o zig-zags. Además, su diseño está pensado para detectar y filtrar los elementos que se consideren “ruido”. Se utiliza con frecuencia en bases de datos espaciales. No obstante, también tiene limitaciones, ya que puede tener dificultades en datasets con densidad variable en diferentes regiones o con datasets representados en espacios de alta dimensionalidad.\n\n\n\n2.1.2 Clustering jerárquico\nEn el clustering jerárquico, los agrupamientos están anidados y pueden formar una estructura jerárquica, que se suele representar gráficamente en forma de un árbol llamado dendograma, como el que se presenta en la Figura 2.4.\n\n\n\n\n\n\nFigura 2.4: Ejemplo de un dendograma que representa los agrupamientos encontrados tras aplicar un algoritmo de clustering jerárquico. Fuente: Figura 21.1 de (Boehmke & Greenwell, 2019).\n\n\n\nDebemos tener presente que, en función del nivel en el que “cortemos” el dendograma, obtendremos un número diferente de grupos. Por ejemplo, la Figura 2.5\n\n\n\n\n\n\nFigura 2.5: Tres dendogramas que muestran diferentes cortes a distinto nivel del árbol. Izq.: dendograma original (sin cortes); (centro) corte a nivel 9, que genera dos agrupamientos; (dcha.) corte a nivel 5, que define 3 agrupamientos. Fuente: Figura 12.11 de (James, 2021).\n\n\n\n\n2.1.2.1 AGNES\nEl método de clustering aglomerativo parte de un conjunto de \\(n\\) grupos, uno por cada elemento de nuestro dataset. En cada iteración, une los dos grupos más similares hasta que se obtenga un único grupo que contiene a todos los datos.\nPara ello, se necesita establecer un criterio de “enlace” para fusionar los grupos, que determine la distancia entre dos agrupamientos. Se pueden usar varios criterios:\n\nAgrupamiento de enlace completo: minimiza la distancia máxima entre las parejas de observaciones de dos agrupamientos.\nAgrupamiento de enlace promedio: minimizar la media de las distancias entre las observaciones pertenecientes a dos clusteres (calculadas por pares).\nAgrupamiento de enlace mínimo o simple: minimizar las distancias entre las observaciones más cercanas en dos clústers.\nMétodo de Ward: minimizar la suma de las diferencias cuadráticas dentro de los clusteres. De ese modo, se minimiza la varianza total del conglomerado.\nEnlace de centroides: se calcula la distancia entre el centroide del grupo A y el centroide del grupo B.\n\n\n\n\n2.1.3 Medidas de evaluación en clustering\n\n2.1.3.1 Elbow method\n\n\n2.1.3.2 La silueta (silhouette)\n\n\n2.1.3.3 La brecha (gap)",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aprendizaje máquina no supervisado</span>"
    ]
  },
  {
    "objectID": "02-ml-no-supervisado.html#reducción-de-la-dimensionalidad",
    "href": "02-ml-no-supervisado.html#reducción-de-la-dimensionalidad",
    "title": "2  Aprendizaje máquina no supervisado",
    "section": "2.2 Reducción de la dimensionalidad",
    "text": "2.2 Reducción de la dimensionalidad\n\n2.2.1 Análisis de componentes principales (PCA)\n\n\n2.2.2 t-distributed Stochastic Neighbor Embedding (t-SNE)\n\n\n2.2.3 Uniform Manifold Approximation and Projection (UMAP)\nSe trata de una técnica reciente de reducción de la dimensionalidad, que puede usarse de modo similar a t-SNE (para representaciones gráficas), pero también como herramienta no lineal de reducción de las dimensiones en el espacio de representación.\n\nManual oficial de UMAP.\n\nLos datos deben cumplir tres supuestos iniciales:\n\nLos datos se distribuyen de modo uniforme en un manifold de Riemann.\nLa métrica de Riemann es constante localmente (o se puede aproximar a una constante).\nEl manifold está localmente conectado.\n\nEl paquete umap en R implementa este algoritmo. En la página de descripción en GitHub se encuentran algunos ejemplos de visualizaciones con UMAP para el dataset MNIST de dígitos.\nSiendo un algoritmo que incluye componentes estocásticas (igual que t-SNE), no se puede garantizar que el resultado de la proyección obtenida sea reproducible. No obstante, se pueden fijar ciertos valores que permiten obtener resultados reproducibles, a costa de perder eficiencia.\nLa documentación oficial también incluye varios ejemplos de cómo podemos usar UMAP como herramienta de clustering.\n\n\n\n\nBoehmke, B., & Greenwell, B. (2019). Hands-On Machine Learning with R (1.ª ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377\n\n\nJames, W., G. (2021). An Introduction to Statistical Learning with Applications in R (2.ª ed.). Springer. https://www.statlearning.com/\n\n\nTan, P.-N., Steinbach, M. S., Karpatne, A., & Kumar, V. (2019). Introduction to Data Mining (Second Edition). Pearson.",
    "crumbs": [
      "Fundamentos",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Aprendizaje máquina no supervisado</span>"
    ]
  },
  {
    "objectID": "03-clust-part.html",
    "href": "03-clust-part.html",
    "title": "3  Clustering particionado",
    "section": "",
    "text": "3.1 El algoritmo k-means\nA la hora de implementar en la práctica el algoritmo k-means, debemos tener en cuenta varios aspectos importantes.",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering particionado</span>"
    ]
  },
  {
    "objectID": "03-clust-part.html#el-algoritmo-k-means",
    "href": "03-clust-part.html#el-algoritmo-k-means",
    "title": "3  Clustering particionado",
    "section": "",
    "text": "Ubicación de partida de los centroides: el algoritmo es sensible a la ubicación inicial de los k centroides para cada grupo. Una mala ubicación puede ofrecer un resultado nada óptimo o, incluso, provocar que el algoritmo no converja en un número razonable de iteraciones. Para resolver este problema, se ha propuesto el algoritmo k-means++ (Arthur & Vassilvitskii, 2007), que inicializa los centroides de forma que se asegure que la solución obtenida es razonablemente óptima.\nFunción de distancia Euclídea: Como ya se ha avisado, la mayoría de implementaciónes de k-means en paquetes software y bibliotecas asumen que se va a emplear la distancia Euclídea para comparar los elementos, estableciendo como objetivo minimizar la WCSS.\nMétodo para elegir el número de agrupamientos k: puesto que no se puede saber a priori el mejor valor de k, se suele aconsejar emplear algún tipo de métrica de evaluación para ponderar la bondad de los grupo generados, comparando los resultados para diversos valores de k.\n\n\n3.1.1 Implementaciones y mejoras en k-means\nOtro aspecto poco conocido pero muy interesante es la existencia de un gran número de implementaciones para resolver el algoritmo k-means, que han ido surgiendo para acelerar el cómputo o ahorrar muchas operaciones (Wang et al., 2020). La Figura 3.1 muestra una línea temporal que describe algunas de las mejoras más sobresalientes que se han ido desarrollando.\n\n\n\n\n\n\nFigura 3.1: Línea temporal que muestra las implementaciones y mejoras más destacadas que se han desarrollado para resolver el problema del agrupamiento con k-means. Fuente: (Wang et al., 2020).\n\n\n\nEs conveniente, siempre que sea posible, informarnos bien acerca de la implementación específica que integra el software que vamos a utilizar para resolver k-means, o si existen varias alternativas cuál de ellas es la más propicia para nuestros intereses.\n\n\n3.1.2 Ejemplo práctico en R\nEjemplo tomado del Capítulo 12 de (Nwanganga & Chapple, 2020).\nDataset: college.csv\nEste conjunto de datos contiene información sobre una lista de facultades y universidades en EE.UU. Los datos provienen del Departamento Estadounidense de Educación y se han filtrado y modificado para este ejemplo (Nwanganga & Chapple, 2020). En total, contiene 1.270 observaciones.\nLas características son:\n\nid: integer, id unívoco para cada observación.\nname: nombre de la institución.\ncity: nombre de la ciudad en la que se ubica la institución.\nstate: abreviatura estándar con dos caracteres del nombre del estado en el que se encuentra la institución.\nregion: una de las cuatro regiones en las que está ubicada la institución. Valores: \"Northeast\", \"Midwest\", \"West\", \"South\".\nhighest_degree: es el máximo nivel de titulación ofrecido por la institución. Valores: \"Associate\", \"Bachelor\", \"Graduate\", \"Nondegree\".\ncontrol: naturaleza de la institución. Valores: \"Public\", \"Private\".\ngender: género de los estudiantes en la institución. Valores: \"CoEd\", \"Male\", \"Female\".\nadmission_rate: porcentaje del total de estudiantes que lo solicitan que son admitidos por la institución.\nsat_avg: puntuación promedio de los solicitantes en el test SAT (rango: 400 a 1600).\nundergrads: número de estudiantes de pregrado en la institución.\ntuition: coste de matriculación anual en la institución, en USD.\nfaculty_salary-avg: salario mensual promedio de los miembros del claustro, en dólares.\nloan_default_rate: porcentaje de estudiantes que, más tarde, no pueden hacer frente a los pagos del préstamo de su matrícula.\nmedian_debt: mediana del montante de la deuda contraída por estudiantes graduados, en USD.\nlong: coordenada de longitud del campus principal.\nlat: coordenada de latitud del campus principal.\n\nObjetivo: segmentar las facultades en grupos de observaciones similares entre sí, mediante el algoritmo k-means. Para acelerar la ejecución del ejemplo, se limitan los cálculos a las facultades ubicadas en el estado de Maryland, aunque los mismos procedimientos pueden aplicarse a otros subconjuntos de los datos.\n\n# Install the packages needed for this code file\n# install.packages(c(\"stats\",\"factoextra\",\"gridExtra\",\"cluster\"))\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(gridExtra)\nlibrary(cluster)\n\n# Load and preview the colleges and universities dataset.\ncollege &lt;- read_csv(\"data/college.csv\", col_types = \"nccfffffnnnnnnnnn\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n# Get a preview of the data.\nglimpse(college)\n\nRows: 1,270\nColumns: 17\n$ id                 &lt;dbl&gt; 102669, 101648, 100830, 101879, 100858, 100663, 101…\n$ name               &lt;chr&gt; \"Alaska Pacific University\", \"Marion Military Insti…\n$ city               &lt;chr&gt; \"Anchorage\", \"Marion\", \"Montgomery\", \"Florence\", \"A…\n$ state              &lt;fct&gt; AK, AL, AL, AL, AL, AL, AL, AL, AL, AL, AL, AL, AL,…\n$ region             &lt;fct&gt; West, South, South, South, South, South, South, Sou…\n$ highest_degree     &lt;fct&gt; Graduate, Associate, Graduate, Graduate, Graduate, …\n$ control            &lt;fct&gt; Private, Public, Public, Public, Public, Public, Pu…\n$ gender             &lt;fct&gt; CoEd, CoEd, CoEd, CoEd, CoEd, CoEd, CoEd, CoEd, CoE…\n$ admission_rate     &lt;dbl&gt; 0.4207, 0.6139, 0.8017, 0.6788, 0.8347, 0.8569, 0.8…\n$ sat_avg            &lt;dbl&gt; 1054, 1055, 1009, 1029, 1215, 1107, 1041, 1165, 107…\n$ undergrads         &lt;dbl&gt; 275, 433, 4304, 5485, 20514, 11383, 7060, 3033, 264…\n$ tuition            &lt;dbl&gt; 19610, 8778, 9080, 7412, 10200, 7510, 7092, 27324, …\n$ faculty_salary_avg &lt;dbl&gt; 5804, 5916, 7255, 7424, 9487, 9957, 6801, 8367, 743…\n$ loan_default_rate  &lt;dbl&gt; 0.077, 0.136, 0.106, 0.111, 0.045, 0.062, 0.096, 0.…\n$ median_debt        &lt;dbl&gt; 23250.0, 11500.0, 21335.0, 21500.0, 21831.0, 21941.…\n$ lon                &lt;dbl&gt; -149.90028, -87.31917, -86.29997, -87.67725, -85.48…\n$ lat                &lt;dbl&gt; 61.21806, 32.63235, 32.36681, 34.79981, 32.60986, 3…\n\n\nPreparamos el dataset.\n\n# Let's limit our dataset to only colleges in the state of Maryland.\n# We also convert the name of each college to the row labels.\nmaryland_college &lt;- college %&gt;%\n  filter(state == \"MD\") %&gt;%\n  column_to_rownames(var = \"name\")\n\n# Let's take a look at the summary stats for the dataset.\nmaryland_college %&gt;%\n  select(admission_rate, sat_avg) %&gt;%\n  summary()\n\n admission_rate      sat_avg    \n Min.   :0.1608   Min.   : 842  \n 1st Qu.:0.5181   1st Qu.: 900  \n Median :0.5961   Median :1048  \n Mean   :0.5886   Mean   :1062  \n 3rd Qu.:0.6606   3rd Qu.:1176  \n Max.   :0.8696   Max.   :1439  \n\n# The summary statistics show a wide range of values for our features.\n# In order to avoid features with large ranges from dominating our model,\n# we need to normalize the features using the scale() function for z-score normalization.\nmaryland_college_scaled &lt;- maryland_college %&gt;%\n  select(admission_rate, sat_avg) %&gt;%\n  scale()\n\n# What do we have now?\nmaryland_college_scaled %&gt;%\n  summary()\n\n admission_rate        sat_avg       \n Min.   :-2.77601   Min.   :-1.2512  \n 1st Qu.:-0.45725   1st Qu.:-0.9218  \n Median : 0.04895   Median :-0.0813  \n Mean   : 0.00000   Mean   : 0.0000  \n 3rd Qu.: 0.46753   3rd Qu.: 0.6485  \n Max.   : 1.82387   Max.   : 2.1393  \n\n\nEjecutamos k-means\n\n# We are now ready to attempt to cluster the data.\n# We set the value for k to 3 and choose to use 25 different initial configurations.\n# The use of set.seed() ensures that every time we run our code, we get the same results.\nset.seed(1234)\nk_3 &lt;- kmeans(maryland_college_scaled, centers=3, nstart = 25)\n\nEvaluamos los resultados\n\n# Let's take a look at the size of the clusters...\nk_3$size\n\n[1] 2 9 8\n\n# ...and the cluster centers.\nk_3$centers\n\n  admission_rate    sat_avg\n1     -1.7425275  1.7871932\n2     -0.2001854 -0.8322366\n3      0.6608405  0.4894679\n\n\n\n# We can also visualize the clusters to get additional insight.\nfviz_cluster(k_3,\n             data = maryland_college_scaled,\n             repel = TRUE,\n             ggtheme = theme_minimal()) + theme(text = element_text(size = 14))\n\n\n\n\n\n\n\nFigura 3.2\n\n\n\n\n\n\n# To further evaluate our results, we need to look at how attributes vary by cluster.\nmaryland_college %&gt;%\n  mutate(cluster = k_3$cluster) %&gt;%\n  select(cluster,\n         undergrads,\n         tuition,\n         faculty_salary_avg,\n         loan_default_rate,\n         median_debt) %&gt;%\n  group_by(cluster) %&gt;%\n  summarise_all(\"mean\")\n\n# A tibble: 3 × 6\n  cluster undergrads tuition faculty_salary_avg loan_default_rate median_debt\n    &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;              &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1       1     16286.  28244.             11258             0.0175      17875 \n2       2      3407   14219.              7781.            0.108       24776.\n3       3      4711.  27523.              7593.            0.045       23925.\n\n\n\n# Let's see how varying the number of clusters affects the results.\nk_4 &lt;- kmeans(maryland_college_scaled, centers = 4, nstart = 25)\nk_5 &lt;- kmeans(maryland_college_scaled, centers = 5, nstart = 25)\nk_6 &lt;- kmeans(maryland_college_scaled, centers = 6, nstart = 25)\n\n# Plot and compare the results.\np1 &lt;- fviz_cluster(k_3, geom = \"point\", data = maryland_college_scaled) + ggtitle(\"k = 3\")\np2 &lt;- fviz_cluster(k_4, geom = \"point\", data = maryland_college_scaled) + ggtitle(\"k = 4\")\np3 &lt;- fviz_cluster(k_5, geom = \"point\", data = maryland_college_scaled) + ggtitle(\"k = 5\")\np4 &lt;- fviz_cluster(k_6, geom = \"point\", data = maryland_college_scaled) + ggtitle(\"k = 6\")\n\n# Here, we make use of the grid.arrange() function in the gridExtra package to display several plots at the same time.\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, nrow = 2)\n\n\n\n\n\n\n\nFigura 3.3\n\n\n\n\n\n\n\n\n\n\n\nPregunta: optimizando el número de centroides\n\n\n\nA partir de los resultados del gráfico anterior, ¿puedes decir cuál es el número de centroides óptimo para el agrupamiento de estos datos?\n\n\n\n# Now let's try to choose an ideal value for k based on the elbow method.\n# Note: The geom_point() function is used to create additional circles on the plot.\nfviz_nbclust(maryland_college_scaled, kmeans, method = \"wss\") +\n  geom_point(\n    shape = 1,\n    x = 4,\n    y = 7.3,\n    colour = \"red\",\n    size = 8,\n    stroke = 1.5\n  ) + \n  geom_point(\n    shape = 1,\n    x = 7,\n    y = 2.3,\n    colour = \"red\",\n    size = 8,\n    stroke = 1.5\n  )\n\n\n\n\n\n\n\nFigura 3.4\n\n\n\n\n\n\n# What about based on the silhouette method?\nfviz_nbclust(maryland_college_scaled, kmeans, method = \"silhouette\") +\n  geom_point(\n    shape = 1,\n    x = 4,\n    y = 0.393,\n    colour = \"red\",\n    size = 8,\n    stroke = 1.5\n  ) +\n  geom_point(\n    shape = 1,\n    x = 7,\n    y = 0.375,\n    colour = \"red\",\n    size = 8,\n    stroke = 1.5\n  )\n\n\n\n\n\n\n\nFigura 3.5\n\n\n\n\n\n\n# Now, let's see what the gap statistic tells us.\nfviz_nbclust(maryland_college_scaled, kmeans, method = \"gap_stat\") +\n  geom_point(\n    shape = 1,\n    x = 1,\n    y = 0.218,\n    colour = \"red\",\n    size = 8,\n    stroke = 1.5\n  ) +\n  geom_point(\n    shape = 1,\n    x = 7,\n    y = 0.2,\n    colour = \"red\",\n    size = 8,\n    stroke = 1.5\n  )\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\nFigura 3.6\n\n\n\n\n\n\n# Alternatively, we can use fviz_gap_stat() function to generate the similar plot for gap statistics.\n# Note: The two plot may not be exactly alike because fviz_nbclust() function does not specify the nstart argument. \nset.seed(1234)\ngap_stat &lt;- clusGap(maryland_college_scaled, FUN = kmeans, nstart = 25, K.max = 10, B = 10)\nfviz_gap_stat(gap_stat)\n\n\n\n\n\n\n\nFigura 3.7\n\n\n\n\n\n\n\n\n\n\n\nOtra forma de calcular k\n\n\n\nA partir de los resultados del gráfico anterior, ¿puedes decir cuál es el número de centroides óptimo para el agrupamiento de estos datos?\n\n\n\nk_4 &lt;- kmeans(maryland_college_scaled, centers = 4, nstart = 25)\n\nfviz_cluster(\n  k_4,\n  data = maryland_college_scaled,\n  main = \"Maryland Colleges Segmented by SAT Scores and Admission Rates\",\n  repel = TRUE,\n  ggtheme = theme_minimal()\n) +\n  theme(text = element_text(size = 14))\n\n\n\n\n\n\n\nFigura 3.8\n\n\n\n\n\n\n\n\n\n\n\nEjercicios adicionales\n\n\n\n\nRepite los cálculos anteriores para las escuelas ubicadas en el estado de Indinana. Céntrate en las variables faculty_salary-avg y tuition. Elige k=3 para visualizar los grupos identificados.\nEmplea las técnicas descritas arriba para elegir el número óptimo de clústeres en el problema del apartado 1). Justifica tu respuesta.\nGenera diagramas de agrupamiento para los dos posibles valores de k que has seleccionado en el apartado 2) anterior. ¿Cuál piensas que es el valor óptimo y por qué?",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering particionado</span>"
    ]
  },
  {
    "objectID": "03-clust-part.html#pam-el-algoritmo-k-medoids",
    "href": "03-clust-part.html#pam-el-algoritmo-k-medoids",
    "title": "3  Clustering particionado",
    "section": "3.2 PAM: el algoritmo k-medoids",
    "text": "3.2 PAM: el algoritmo k-medoids\nEl algoritmo k-medoids tiene algunas importantes ventajas respecto de k-means:\n\nLos prototipos de cada grupo no son centroides (puntos ficticios o sintéticos) sino medoides, es decir, elementos reales* del dataset. Esto facilita mucho la interpretación de los resultados, puesto que se puede analizar las características de cada elemento prototipo para concluir cuáles son los principales rasgos de los elementos de ese grupo.\nMientras que en muchas implementaciones de k-means, se asume que vamos a emplear la función de distancia Euclídea y que se busca minimizar la WCSS para resolver el problema, en las implementaciones de k-medoids se pueden utilizar funciones de distancia arbitrarias para comparar y agrupar los elementos.\n\nTambién existen implementaciones con rendimiento mejorado de este algoritmo. Por ejemplo, el paquete fastkmedoids en R proporciona wrappers para las implementaciones en C++ de varios algoritmos más rápidos para resolver el problema PAM (FastPAM, FastCLARA Y FastCLARANS) (Schubert & Rousseeuw, 2019).\n\n3.2.1 Ejemplo de implementación en R\n\n# 1. Cargar librerías necesarias\n# Si no tienes fastkmedoids: install.packages(\"fastkmedoids\")\nlibrary(fastkmedoids)\nlibrary(ggplot2)\nlibrary(ggrepel) # Opcional: para que las etiquetas de texto no se solapen\n\n# 2. Cargar y preparar los datos\ndata(\"USArrests\")\n\n# Es fundamental escalar los datos porque 'Assault' tiene valores mucho más grandes\n# que 'Murder', lo que sesgaría el cálculo de distancias.\ndf_scaled &lt;- scale(USArrests)\n\n# fastpam requiere una matriz de distancias, no el dataframe directo\ndist_matrix &lt;- dist(df_scaled, method = \"euclidean\")\n\n# 3. Ejecutar fastpam (K-Medoids rápido)\nset.seed(123)\nk &lt;- 4 # Elegimos 4 grupos para este ejemplo\nresultado &lt;- fastpam(dist_matrix, n=nrow(df_scaled), k = k)\n\n# El objeto 'resultado' tiene:\n# resultado@medoids: los índices (filas) de los estados que son medoides\n# resultado@assignment: el grupo asignado a cada estado\n# resultado@cost: valor final de la función de coste\n\n# 4. Reducción de dimensión para visualización (PCA)\n# Necesitamos pasar de 4 variables a 2 ejes (PC1 y PC2) para poder graficar\npca &lt;- prcomp(df_scaled)\npca_data &lt;- as.data.frame(pca$x[, 1:2]) # Tomamos las dos primeras componentes\n\n# 5. Construimos el data.frame para el gráfico\n# Añadimos la información del cluster y si es medoide o no\npca_data$Cluster &lt;- as.factor(resultado@assignment)\npca_data$State   &lt;- rownames(USArrests)\npca_data$IsMedoid &lt;- FALSE\n\n# Marcamos cuáles filas son los medoides reales encontrados por fastpam\npca_data$IsMedoid[resultado@medoids] &lt;- TRUE\n\n# 6. Creación del gráfico con ggplot2\nggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +\n  \n  # A. Dibujar los puntos normales (Estados)\n  geom_point(alpha = 0.6, size = 2) +\n  \n  # B. Dibujar los Medoides (Más grandes y con forma diferente)\n  geom_point(data = subset(pca_data, IsMedoid == TRUE), \n             size = 5, shape = 18, stroke = 2) +\n  \n  # C. Etiquetas para los medoides (Nombres de los estados representativos)\n  geom_label_repel(data = subset(pca_data, IsMedoid == TRUE),\n                   aes(label = State),\n                   box.padding = 0.5, show.legend = FALSE) +\n  \n  # D. Elipses para visualizar las áreas de los grupos (Opcional)\n  stat_ellipse(aes(fill = Cluster), geom = \"polygon\", alpha = 0.1) +\n  \n  # Estética general\n  theme_minimal() +\n  labs(title = \"Visualización de Clustering con fastpam (USArrests)\",\n       subtitle = \"Los rombos grandes indican los Medoides (Centros reales de cada grupo)\",\n       x = \"Componente Principal 1\",\n       y = \"Componente Principal 2\")\n\n\n\n\n\n\n\nFigura 3.9: Resultado del clustering con fastkmedoids sobre el dataset USArrests.\n\n\n\n\n\n\n\n\n\nArthur, D., & Vassilvitskii, S. (2007). k-means++: the advantages of careful seeding. En N. Bansal, K. Pruhs, & C. Stein (Eds.), Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2007, New Orleans, Louisiana, USA, January 7-9, 2007 (pp. 1027-1035). SIAM.\n\n\nNwanganga, F., & Chapple, M. (2020). Practical Machine Learning in R (1.ª ed.). John Wiley & Sons, Inc. https://www.wiley.com/en-us/Practical+Machine+Learning+in+R-p-9781119591535\n\n\nSchubert, E., & Rousseeuw, P. J. (2019). Faster k-Medoids Clustering: Improving the PAM, CLARA, and CLARANS Algorithms. En G. Amato, C. Gennaro, V. Oria, & M. Radovanović (Eds.), Similarity Search and Applications (pp. 171-187). Springer International Publishing.\n\n\nWang, S., Sun, Y., & Bao, Z. (2020). On the efficiency of K-means clustering: evaluation, optimization, and algorithm selection. Proc. VLDB Endow., 14(2), 163-175. https://doi.org/10.14778/3425879.3425887",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Clustering particionado</span>"
    ]
  },
  {
    "objectID": "04-clust-jer.html",
    "href": "04-clust-jer.html",
    "title": "4  Clustering jerárquico",
    "section": "",
    "text": "4.1 AGNES\nVeamos un ejemplo con el dataset USArrests. La función cluster::agnes implementa este algoritmo en R. También usaremos las funciones de utilidad del paquete factoextra para representación gráfica y comparación de alternativas de agrupamiento utilizando métricas de evaluación del resultado.\nlibrary(cluster)\nlibrary(factoextra)\nlibrary(tidyverse)\ndf &lt;- USArrests\n\n# eliminamos valores faltantes\ndf &lt;- na.omit(df)\n\n# escalamos las variables\ndf &lt;- scale(df)\n\nhead(df)\n\n               Murder   Assault   UrbanPop         Rape\nAlabama    1.24256408 0.7828393 -0.5209066 -0.003416473\nAlaska     0.50786248 1.1068225 -1.2117642  2.484202941\nArizona    0.07163341 1.4788032  0.9989801  1.042878388\nArkansas   0.23234938 0.2308680 -1.0735927 -0.184916602\nCalifornia 0.27826823 1.2628144  1.7589234  2.067820292\nColorado   0.02571456 0.3988593  0.8608085  1.864967207\nLa función que implementa el algoritmo AGNES en R es agnes (ninguna sorpresa). Uno de sus resultados es el campo ac, que mide el coeficiente aglomerativo del resultado. Éste cuantifica la cantidad de estructura de agrupaciones que se ha encontrado (mejores valores cercanos a 1).\n# Clustering jerárquico (enlace completo)\nhc2 &lt;- agnes(df, method = \"complete\" )\nhc2$ac\n\n[1] 0.8531583\nComparemos diferentes métodos de conexión para el agrupamiento jerárquico.\n# Métodos evaluados\nm &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) &lt;- c( \"average\", \"single\", \"complete\", \"ward\")\n\n# Función para calcular el coeficiente de agrupamiento\nac &lt;- function(x) {\n  agnes(df, method = x)$ac\n}\nmap_dbl(m, ac)\n\n  average    single  complete      ward \n0.7379371 0.6276128 0.8531583 0.9346210\nEl siguiente dendograma utiliza un método de enlazado completo.\n# Matriz de disimilaridades\nd &lt;- dist(df, method = \"euclidean\")\n\n# Clustering jerárquico usando enlace completo\nhc1 &lt;- hclust(d, method = \"complete\" )\n\n# Dendrograma\nplot(hc1, cex = 0.6, hang = -1)\n\n\n\n\n\n\n\nFigura 4.1\nSin embargo, si cambiamos al método de enlazado de Ward obtenemos resultados diferentes.\nhc2 &lt;- agnes(df, method = \"ward\" )\n\n# Drendrograma\npltree(hc2, cex = 0.6, hang = -1, main = \"Dendrograma de AGNES (método Ward)\") \n\n\n\n\n\n\n\nFigura 4.2\nEn función de a qué altura cortemos el árbol, obtenemos diferentes agrupamientos.\n# Método de Ward\nhc5 &lt;- hclust(d, method = \"ward.D2\" )\n\n# Cortamos en 4 clusters\nsub_grp &lt;- cutree(hc5, k = 4)\n\n# Visualizamos el corte en el dendrograma\nplot(hc5, cex = 0.6)\nrect.hclust(hc5, k = 4, border = 2:5)\n\n\n\n\n\n\n\nFigura 4.3\nPodemos ver el número de observaciones en cada grupo.\ntable(sub_grp)\n\nsub_grp\n 1  2  3  4 \n 7 12 19 12\nTambién se puede representar gráficamente el resultado, ya que la función fviz_cluster acepta este tipo de objetos.\nfviz_cluster(list(data=df,cluster=sub_grp))\n\n\n\n\n\n\n\nFigura 4.4\nAdemás, podemos usar fviz_nbclust para utilizar un método de evaluación de los agrupamientos obtenidos con diferentes valores de grupos, y así decidir la mejor opción de configuración (en este caso, a qué altura cortamos el árbol). En el siguiente ejemplo se usa la WCSS.\nfviz_nbclust(df, FUN = hcut, method = \"wss\")\n\n\n\n\n\n\n\nFigura 4.5\nO también el método de la silueta. Métodos diferentes de evaluación pueden arrojar conclusiones distintas.\nfviz_nbclust(df, FUN = hcut, method = \"silhouette\")\n\n\n\n\n\n\n\nFigura 4.6",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Clustering jerárquico</span>"
    ]
  },
  {
    "objectID": "04-clust-jer.html#dbscan",
    "href": "04-clust-jer.html#dbscan",
    "title": "4  Clustering jerárquico",
    "section": "4.2 DBSCAN",
    "text": "4.2 DBSCAN\n\n4.2.1 Ejemplo básico con R\n\nlibrary(dbscan)\n\n\nAdjuntando el paquete: 'dbscan'\n\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nlibrary(ggplot2)\n\n# Datos de ejemplo\ndata(\"moons\", package = \"dbscan\")\n\n# Ejecución del algoritmo\n# eps: radio de búsqueda\n# minPts: puntos mínimos para considerar núcleo\nres &lt;- dbscan(moons, eps = 0.3, minPts = 5)\n\n# Visualización\nggplot(moons, aes(X, Y, color = factor(res$cluster))) +\n  geom_point() +\n  labs(title = \"Resultado DBSCAN\", color = \"Cluster (0 = Ruido)\") +\n  theme_minimal()",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Clustering jerárquico</span>"
    ]
  },
  {
    "objectID": "05-mapas-auto-org.html",
    "href": "05-mapas-auto-org.html",
    "title": "5  Mapas auto-organizados",
    "section": "",
    "text": "5.1 Ejemplo con R\nEjemplos de algoritmos de cada clase\nEl paquete kohonen implementa en R algoritmos SOM, incluyendo también herramientas para interrogar al mapa resultante y obtener predicciones.\nEn el ejemplo, vamos a utilizar el dataset nutrients.meat.fish.fowl.1959, que está incluido en el paquete cluster.datasets.\nlibrary(\"kohonen\")\nlibrary(\"cluster.datasets\")\nlibrary(\"ggplot2\")\nEl conjunto de datos nutrients.meat.fish.fowl.1959 contiene información nutricional (calorías, proteínas, grasas, calcio, hierro) de diferentes tipos de carne. Nuestro objetivo al aplicar un SOM será agrupar estas carnes en un mapa 2D, de tal forma que las carnes nutricionalmente similares queden cerca unas de otras.\n# 1. Cargar el dataset\ndata(\"nutrients.meat.fish.fowl.1959\")\ndf &lt;- nutrients.meat.fish.fowl.1959\n\n# Miremos la estructura\nhead(df)\n\n             name energy protein fat calcium iron\n1    Braised beef    340      20  28       9  2.6\n2       Hamburger    245      21  17       9  2.7\n3      Roast beef    420      15  39       7  2.0\n4       Beefsteak    375      19  32       9  2.6\n5     Canned beef    180      22  10      17  3.7\n6 Broiled chicken    115      20   3       8  1.4\n\n# 2. Preprocesamiento\n# El nombre de la carne (columna 'name') no es un dato numérico para el cálculo,\n# pero lo necesitamos para etiquetar el gráfico al final.\nnombres_carnes &lt;- df$name\n\n# Seleccionamos solo las columnas numéricas (Nutrientes)\n# Según la estructura usual de este dataset, la columna 1 es el nombre.\ndata_matrix &lt;- as.matrix(df[, -1])\n\n# 3. Escalado (NORMALIZACIÓN) - ¡Paso Crítico!\n# Los SOM funcionan con distancias. Si las Calorías son 300 y el Hierro es 2,\n# la variable Calorías dominará todo el mapa. Debemos escalar para que todas\n# las variables tengan el mismo peso.\ndata_scaled &lt;- scale(data_matrix)",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mapas auto-organizados</span>"
    ]
  },
  {
    "objectID": "05-mapas-auto-org.html#ejemplo-con-r",
    "href": "05-mapas-auto-org.html#ejemplo-con-r",
    "title": "5  Mapas auto-organizados",
    "section": "",
    "text": "5.1.1 Creación y entrenamiento del SOM\nAhora, definimos la rejilla (grid) en la que vamos a proyectar los datos. La mejor opción suele ser un mapa hexagonal.\n\nset.seed(123) # Para reproducibilidad\n\n# 1. Definir la rejilla (Grid)\n# Usaremos un mapa pequeño de 5x5 hexágonos (25 neuronas en total)\n# dado que el dataset no es masivo.\nsom_grid &lt;- somgrid(xdim = 5, ydim = 5, topo = \"hexagonal\")\n\n# 2. Entrenar el modelo\n# rlen: número de veces que el dataset completo se presenta a la red.\nsom_model &lt;- som(data_scaled, \n                 grid = som_grid, \n                 rlen = 500, \n                 alpha = c(0.05, 0.01), # Tasa de aprendizaje (inicio, fin)\n                 keep.data = TRUE)\n\n# Ver resumen del modelo\nsummary(som_model)\n\nSOM of size 5x5 with a hexagonal topology and a bubble neighbourhood function.\nThe number of data layers is 1.\nDistance measure(s) used: sumofsquares.\nTraining data included: 27 objects.\nMean distance to the closest unit in the map: 0.306.\n\n\n\n\n5.1.2 Representación gráfica de los resultados\n\nplot(som_model, type = \"changes\", main = \"Progreso del Entrenamiento\")\n\n\n\n\n\n\n\nFigura 5.1\n\n\n\n\n\n\nplot(som_model, type = \"counts\", main = \"Densidad de Nodos (Número de carnes)\")\n\n\n\n\n\n\n\nFigura 5.2\n\n\n\n\n\n\nplot(som_model, type = \"codes\", main = \"Perfil Nutricional por Nodo\", \n     palette.name = rainbow)\n\n\n\n\n\n\n\nFigura 5.3\n\n\n\n\n\n\nplot(som_model, type = \"dist.neighbours\", main = \"U-Matrix (Distancia entre vecinos)\")\n\n\n\n\n\n\n\nFigura 5.4\n\n\n\n\n\nInterpretación:\n\nColores oscuros: Distancia baja. Los nodos vecinos son muy similares.\nColores claros: Distancia alta (fronteras). Indican una separación fuerte entre grupos de datos (clusters naturales).\n\n\n# Asignamos colores bonitos para los clusters\n# Usamos clusters jerárquicos sobre los vectores de código del SOM\n# para definir 3 grandes grupos (ej: Carnes rojas, Pescados, Aves/Otras)\nsom_cluster &lt;- cutree(hclust(dist(som_model$codes[[1]])), 3)\n\n# Paleta de colores para los 3 grupos\nmy_pal &lt;- c(\"red\", \"blue\", \"green\")\nbg_colors &lt;- my_pal[som_cluster]\n\n# Graficamos el mapa final\nplot(som_model, type = \"mapping\", pchs = 21, bg = bg_colors, shape = \"straight\", \n     main = \"Mapa de Carnes (Clustering)\")\n\n# Añadimos el texto (nombres de las carnes) al mapa\nadd.cluster.boundaries(som_model, som_cluster)\n\n\n\n\n\n\n\nFigura 5.5",
    "crumbs": [
      "Clustering",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Mapas auto-organizados</span>"
    ]
  },
  {
    "objectID": "06-reduccion-dim.html",
    "href": "06-reduccion-dim.html",
    "title": "6  Reducción de la dimensionalidad",
    "section": "",
    "text": "6.1 PCA\nEjemplos de algoritmos de cada clase.\nUtilizaremos el dataset estándar USArrests que proporciona R. Seguiremos el ejemplo del laboratorio del Capítulo 12 de (James, 2021) (Sec. 12.5.1).\n## Principal Components Analysis\nstates &lt;- row.names(USArrests)\nstates\n\n [1] \"Alabama\"        \"Alaska\"         \"Arizona\"        \"Arkansas\"      \n [5] \"California\"     \"Colorado\"       \"Connecticut\"    \"Delaware\"      \n [9] \"Florida\"        \"Georgia\"        \"Hawaii\"         \"Idaho\"         \n[13] \"Illinois\"       \"Indiana\"        \"Iowa\"           \"Kansas\"        \n[17] \"Kentucky\"       \"Louisiana\"      \"Maine\"          \"Maryland\"      \n[21] \"Massachusetts\"  \"Michigan\"       \"Minnesota\"      \"Mississippi\"   \n[25] \"Missouri\"       \"Montana\"        \"Nebraska\"       \"Nevada\"        \n[29] \"New Hampshire\"  \"New Jersey\"     \"New Mexico\"     \"New York\"      \n[33] \"North Carolina\" \"North Dakota\"   \"Ohio\"           \"Oklahoma\"      \n[37] \"Oregon\"         \"Pennsylvania\"   \"Rhode Island\"   \"South Carolina\"\n[41] \"South Dakota\"   \"Tennessee\"      \"Texas\"          \"Utah\"          \n[45] \"Vermont\"        \"Virginia\"       \"Washington\"     \"West Virginia\" \n[49] \"Wisconsin\"      \"Wyoming\"       \n\nnames(USArrests)\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\n## Computamos la media y la varianza de las columnas\napply(USArrests, 2, mean)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\napply(USArrests, 2, var)\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\n## Calculamos las componentes principales mediante la función `prcomp`\npr.out &lt;- prcomp(USArrests, scale = TRUE)\nnames(pr.out)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"\nLas variables center y scale corresponden a la media y desviación estándar de las variables que se emplearon para el escalado, antes de la transformación. Además, la matriz rotation contiene los pesos de las componentes principales calculadas.\npr.out$center\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\npr.out$scale\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\n###\ndim(pr.out$x)\n\n[1] 50  4\nRepresentamos un diagrama con las dos primeras componentes principales\n###\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\n\nFigura 6.1\nDebemos corregir el signo de las componentes principales para que el gráfico se vea mejor.\n###\npr.out$rotation = -pr.out$rotation\npr.out$x = -pr.out$x\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\n\nFigura 6.2\nPodemos calcular también el porcentaje de varianza explicada.\n###\npr.out$sdev\n\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\n###\npr.var &lt;- pr.out$sdev^2\npr.var\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n###\npve &lt;- pr.var / sum(pr.var)\npve\n\n[1] 0.62006039 0.24744129 0.08914080 0.04335752\nPodemos representar gráficamente este porcentaje para cada componente, así como el PVE acumulado.\npar(mfrow = c(1, 2))\nplot(pve, xlab = \"Principal Component\",\n    ylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\n    type = \"b\")\nplot(cumsum(pve), xlab = \"Principal Component\",\n    ylab = \"Cumulative Proportion of Variance Explained\",\n    ylim = c(0, 1), type = \"b\")\n\n\n\n\n\n\n\nFigura 6.3",
    "crumbs": [
      "Reducción de dimensionalidad",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reducción de la dimensionalidad</span>"
    ]
  },
  {
    "objectID": "06-reduccion-dim.html#pca",
    "href": "06-reduccion-dim.html#pca",
    "title": "6  Reducción de la dimensionalidad",
    "section": "",
    "text": "6.1.1 t-SNE\nEl paquete Rtsne implementa este algoritmo de reducción de dimensionalidad en R.\n\n# Instalar y cargar librerías necesarias\n# install.packages(\"Rtsne\")\n# install.packages(\"ggplot2\")\nlibrary(Rtsne)\nlibrary(ggplot2)\n\n# Cargar y preparar los datos\ndata(\"USArrests\")\n\n# t-SNE falla si hay filas duplicadas. Aunque USArrests no suele tenerlos,\n# es una buena práctica comprobarlo\ndf_unique &lt;- unique(USArrests)\n\n# t-SNE se basa en distancias. Si una variable tiene valores de 1000 y otra de 1,\n# la primera dominará el cálculo. La función 'scale' normaliza los valores para que\n# todas las variables queden en rangos comparables\n\ndf_scaled &lt;- scale(df_unique)\n\n# Ejemcutamos el algoritmo t-SNE\nset.seed(42) # reproducible\n\n# El parámetro 'perplexity' controla el \"número efectivo de vecinos\".\n# Normalmente se usa 30, pero como USArrests es muy pequeño (50 datos),\n# debemos bajarlo (el máximo recomendado suele ser (N-1)/3).\n# Le asignamos un valor de 10.\ntsne_results &lt;- Rtsne(df_scaled, \n                      dims = 2,            # Queremos proyectar a 2 dimensiones\n                      perplexity = 10,     # Ajustado por el tamaño pequeño del dataset\n                      verbose = TRUE, \n                      max_iter = 1000)\n\nPerforming PCA\nRead the 50 x 4 data matrix successfully!\nOpenMP is working. 1 threads.\nUsing no_dims = 2, perplexity = 10.000000, and theta = 0.500000\nComputing input similarities...\nBuilding tree...\nDone in 0.00 seconds (sparsity = 0.719200)!\nLearning embedding...\nIteration 50: error is 55.413555 (50 iterations in 0.00 seconds)\nIteration 100: error is 54.751037 (50 iterations in 0.00 seconds)\nIteration 150: error is 55.937429 (50 iterations in 0.00 seconds)\nIteration 200: error is 54.711323 (50 iterations in 0.00 seconds)\nIteration 250: error is 52.786834 (50 iterations in 0.00 seconds)\nIteration 300: error is 1.819072 (50 iterations in 0.00 seconds)\nIteration 350: error is 1.011247 (50 iterations in 0.00 seconds)\nIteration 400: error is 0.510596 (50 iterations in 0.00 seconds)\nIteration 450: error is 0.364691 (50 iterations in 0.00 seconds)\nIteration 500: error is 0.331375 (50 iterations in 0.00 seconds)\nIteration 550: error is 0.309632 (50 iterations in 0.00 seconds)\nIteration 600: error is 0.300417 (50 iterations in 0.00 seconds)\nIteration 650: error is 0.293517 (50 iterations in 0.00 seconds)\nIteration 700: error is 0.291822 (50 iterations in 0.00 seconds)\nIteration 750: error is 0.242082 (50 iterations in 0.00 seconds)\nIteration 800: error is 0.238562 (50 iterations in 0.00 seconds)\nIteration 850: error is 0.240463 (50 iterations in 0.00 seconds)\nIteration 900: error is 0.234624 (50 iterations in 0.00 seconds)\nIteration 950: error is 0.230820 (50 iterations in 0.00 seconds)\nIteration 1000: error is 0.237176 (50 iterations in 0.00 seconds)\nFitting performed in 0.06 seconds.\n\n# Etraemos las coordenadas generadas para repres. gráfica\ntsne_plot_data &lt;- data.frame(\n  X = tsne_results$Y[, 1],\n  Y = tsne_results$Y[, 2],\n  State = rownames(df_unique) # Nombres de los estados\n)\n\n\n# Visualización con ggplot2\nggplot(tsne_plot_data, aes(x = X, y = Y)) +\n  geom_point(color = \"steelblue\", size = 3, alpha = 0.8) +\n  geom_text(aes(label = State), vjust = 1.5, size = 3) + # Etiquetas de estados\n  theme_minimal() +\n  labs(\n    title = \"Visualización t-SNE de USArrests\",\n    subtitle = \"Estados agrupados por similitud en tasas de crimen\",\n    x = \"Dimensión t-SNE 1\",\n    y = \"Dimensión t-SNE 2\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\nFigura 6.4\n\n\n\n\n\n\n\n6.1.2 UMAP\nEl paquete umap en R implementa este algoritmo.\n\n# Instalar y cargar librerías\n# install.packages(\"umap\")\n# install.packages(\"ggplot2\")\nlibrary(umap)\nlibrary(ggplot2)\n\n# Cargar y preparar los datos\ndata(\"USArrests\")\n\n# UMAP, como t-SNE, se basa en distancias, por lo que es recomendable\n# escalar los valores de las variables\ndf_scaled &lt;- scale(USArrests)\n\n# Definir y ejecutar el algoritmo UMAP\nset.seed(42) # Para reproducibilidad\n\n# Definimos una configuración específica para datasets pequeños como USArrests (N=50)\ncustom_umap_config &lt;- umap.defaults\ncustom_umap_config$n_neighbors &lt;- 5  # El valor por defecto es 15, pero 5 o 10 es mejor para N=50\ncustom_umap_config$min_dist &lt;- 0.1   # Distancia mínima entre puntos proyectados\ncustom_umap_config$random_state &lt;- 42 # Semilla\n\n# Ejecutar UMAP\numap_results &lt;- umap(df_scaled, config = custom_umap_config)\n\n# 4. Preparar datos para graficar\n# UMAP devuelve el resultado en el slot 'layout'\numap_plot_data &lt;- data.frame(\n  X = umap_results$layout[, 1],\n  Y = umap_results$layout[, 2],\n  State = rownames(USArrests)\n)\n\n# 5. Visualización con ggplot2\nggplot(umap_plot_data, aes(x = X, y = Y)) +\n  geom_point(color = \"darkred\", size = 3, alpha = 0.8) +\n  geom_text(aes(label = State), vjust = 1.5, size = 3) + \n  theme_bw() +\n  labs(\n    title = \"Visualización UMAP de USArrests\",\n    subtitle = \"Los ejes representan las dos dimensiones principales de UMAP\",\n    x = \"Dimensión UMAP 1\",\n    y = \"Dimensión UMAP 2\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\nFigura 6.5\n\n\n\n\n\nAl igual que con t-SNE, los estados que se agrupan en el gráfico UMAP tienen perfiles criminales muy similares (por ejemplo, los estados del Sur con altas tasas de crímenes violentos a menudo se agrupan en una región, mientras que los estados más seguros se agrupan en otra). La ventaja de UMAP es que la distancia entre estos grupos de clústeres a menudo se interpreta como una representación más fiel de la distancia global que la que proporciona t-SNE.\n\n\n\n\nJames, W., G. (2021). An Introduction to Statistical Learning with Applications in R (2.ª ed.). Springer. https://www.statlearning.com/",
    "crumbs": [
      "Reducción de dimensionalidad",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reducción de la dimensionalidad</span>"
    ]
  },
  {
    "objectID": "07-autocodificadores.html",
    "href": "07-autocodificadores.html",
    "title": "7  Autocodificaciores",
    "section": "",
    "text": "7.1 Embeddings\nEjemplos de algoritmos de cada clase",
    "crumbs": [
      "Reducción de dimensionalidad",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Autocodificaciores</span>"
    ]
  },
  {
    "objectID": "07-autocodificadores.html#autocodificadores",
    "href": "07-autocodificadores.html#autocodificadores",
    "title": "7  Autocodificaciores",
    "section": "7.2 Autocodificadores",
    "text": "7.2 Autocodificadores",
    "crumbs": [
      "Reducción de dimensionalidad",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Autocodificaciores</span>"
    ]
  },
  {
    "objectID": "08-apps.html",
    "href": "08-apps.html",
    "title": "8  Aplicaciones",
    "section": "",
    "text": "8.1 Segmentación de clientes de un centro comercial\nEn este caso de estudio, se analizan 200 observaciones correspondientes a clientes de un centro comercial. Cada registro de cliente incluye las siguientes variables:",
    "crumbs": [
      "Aplicaciones",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Aplicaciones</span>"
    ]
  },
  {
    "objectID": "08-apps.html#segmentación-de-clientes-de-un-centro-comercial",
    "href": "08-apps.html#segmentación-de-clientes-de-un-centro-comercial",
    "title": "8  Aplicaciones",
    "section": "",
    "text": "CustomerID: identificador unívoco de cliente.\nGender: género.\nAge: edad.\nIncome: salario anual.\nSpendingScore: puntuación (entre 1 y 100) sobre la propensión de compra del cliente, basado en sus hábitos de compra y otros factores.\n\n\n8.1.1 Resolución con k-means\n\nlibrary(tidyverse)\nlibrary(factoextra)\nlibrary(gridExtra)\nlibrary(cluster)\nlibrary(stringr)\n\nmallcustomers &lt;- read_csv(\"data/mallcustomers.csv\")\nglimpse(mallcustomers)\n\nRows: 200\nColumns: 5\n$ CustomerID    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ Gender        &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Female\", \"Female\", …\n$ Age           &lt;dbl&gt; 19, 21, 20, 23, 31, 22, 35, 23, 64, 30, 67, 35, 58, 24, …\n$ Income        &lt;chr&gt; \"15,000 USD\", \"15,000 USD\", \"16,000 USD\", \"16,000 USD\", …\n$ SpendingScore &lt;dbl&gt; 39, 81, 6, 77, 40, 76, 6, 94, 3, 72, 14, 99, 15, 77, 13,…\n\n\nPreparamos los datos.\n\n# We have a couple of things we need to do to prepare the data.\n# The first is to convert the Gender feature to a factor.\n# The second is to convert the income feature to a number.\n# This requires that we remove both the USD and the \",\" in the data.\nmallcustomers &lt;- mallcustomers %&gt;%\n  mutate(Income = str_replace_all(Income,\" USD\",\"\")) %&gt;%\n  mutate(Income = str_replace_all(Income,\",\",\"\")) %&gt;%\n  mutate(Income = as.numeric(Income))\n\nsummary(mallcustomers)\n\n   CustomerID        Gender               Age            Income      \n Min.   :  1.00   Length:200         Min.   :18.00   Min.   : 15000  \n 1st Qu.: 50.75   Class :character   1st Qu.:28.75   1st Qu.: 41500  \n Median :100.50   Mode  :character   Median :36.00   Median : 61500  \n Mean   :100.50                      Mean   :38.85   Mean   : 60560  \n 3rd Qu.:150.25                      3rd Qu.:49.00   3rd Qu.: 78000  \n Max.   :200.00                      Max.   :70.00   Max.   :137000  \n SpendingScore  \n Min.   : 1.00  \n 1st Qu.:34.75  \n Median :50.00  \n Mean   :50.20  \n 3rd Qu.:73.00  \n Max.   :99.00  \n\n# We intend to segment based on Income and SpendingScore only.\n# So, we remove everything else and normalize our features.\nmallcustomers_scaled &lt;- mallcustomers %&gt;%\n  select(-CustomerID, -Gender, -Age) %&gt;%\n  scale()\n\n# What does the data now look like?\nsummary(mallcustomers_scaled)\n\n     Income         SpendingScore      \n Min.   :-1.73465   Min.   :-1.905240  \n 1st Qu.:-0.72569   1st Qu.:-0.598292  \n Median : 0.03579   Median :-0.007745  \n Mean   : 0.00000   Mean   : 0.000000  \n 3rd Qu.: 0.66401   3rd Qu.: 0.882916  \n Max.   : 2.91037   Max.   : 1.889750  \n\n\nAhora, ejecutamos el algoritmo de clustering.\n\n# Elbow Method\np1 &lt;- fviz_nbclust(mallcustomers_scaled, kmeans, method = \"wss\") + geom_point(\n  shape = 1,\n  x = 6,\n  y = 60,\n  colour = \"red\",\n  size = 8,\n  stroke = 1.5\n) + ggtitle(\"Elbow Method\")\n\n# Silhouette Method\np2 &lt;- fviz_nbclust(mallcustomers_scaled, kmeans, method = \"silhouette\") + geom_point(\n  shape = 1,\n  x = 6,\n  y = 0.53,\n  colour = \"red\",\n  size = 8,\n  stroke = 1.5\n) + ggtitle(\"Silhouette Method\")\n\n# Gap Statistic\np3 &lt;- fviz_nbclust(mallcustomers_scaled, kmeans, method = \"gap_stat\") + geom_point(\n  shape = 1,\n  x = 6,\n  y = 0.57,\n  colour = \"red\",\n  size = 8,\n  stroke = 1.5\n) + ggtitle(\"Gap Statistic\")\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\ngrid.arrange(p1, p2, p3, nrow = 3)\n\n\n\n\n\n\n\nFigura 8.1\n\n\n\n\n\nElegimos \\(k=6\\). Ahora, repetimos el algoritmo con 25 configuraciones iniciales diferentes.\n\n# We set the value for k to 6 and choose to use 25 different initial configurations.\nset.seed(1234)\nk_clust &lt;- kmeans(mallcustomers_scaled, centers = 6, nstart = 25)\n\nfviz_cluster(\n  k_clust,\n  data = mallcustomers_scaled,\n  main = \"Mall Customers Segmented by Income and Spending Score\",\n  repel = TRUE,\n  ggtheme = theme_minimal()\n) + theme(text = element_text(size = 14))\n\n\n\n\n\n\n\nFigura 8.2\n\n\n\n\n\n\n# To further evaluate our results, we need to look at how attributes vary by cluster.\nmallcustomers %&gt;%\n  mutate(cluster = k_clust$cluster) %&gt;%\n  mutate(Male = ifelse(Gender == \"Male\", 1, 0)) %&gt;%\n  mutate(Female = ifelse(Gender == \"Female\", 1, 0)) %&gt;%\n  select(cluster, Male, Female, Age) %&gt;%\n  group_by(cluster) %&gt;%\n  summarise_all(\"mean\")\n\n# A tibble: 6 × 4\n  cluster  Male Female   Age\n    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1       1 0.483  0.517  32.9\n2       2 0.4    0.6    32.2\n3       3 0.543  0.457  41.1\n4       4 0.391  0.609  45.2\n5       5 0.407  0.593  42.7\n6       6 0.409  0.591  25.3\n\n\n\nmallcustomers_tagged &lt;- mallcustomers |&gt;\n  mutate(Cluster = as.factor(k_clust$cluster))\nhead(mallcustomers_tagged)\n\n# A tibble: 6 × 6\n  CustomerID Gender   Age Income SpendingScore Cluster\n       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;  \n1          1 Male      19  15000            39 4      \n2          2 Male      21  15000            81 6      \n3          3 Female    20  16000             6 4      \n4          4 Female    23  16000            77 6      \n5          5 Female    31  17000            40 4      \n6          6 Female    22  17000            76 6      \n\n# Resumen estadístico por grupo\nresumen &lt;- mallcustomers_tagged %&gt;%\n  group_by(Cluster) %&gt;%\n  summarise(\n    Cantidad = n(),\n    Edad_Promedio = mean(Age),\n    Ingreso_Promedio = mean(Income),\n    Spending_Promedio = mean(SpendingScore)\n  )\n\nprint(resumen)\n\n# A tibble: 6 × 5\n  Cluster Cantidad Edad_Promedio Ingreso_Promedio Spending_Promedio\n  &lt;fct&gt;      &lt;int&gt;         &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;\n1 1             29          32.9           78552.              82.2\n2 2             10          32.2          109700               82  \n3 3             35          41.1           88200               17.1\n4 4             23          45.2           26304.              20.9\n5 5             81          42.7           55296.              49.5\n6 6             22          25.3           25727.              79.4\n\n\nAhora, podemos tratar de identificar algunos perfiles frecuentes de clientes:\n\nAhorradores: ingresos altos, pero bajo SpendingScore.\nDerrochadores: ingresos bajos, pero alto SpendingScore.\nClientes premium* *: ingresos altos y alto SpendingScore (este sería el grupo más valioso para los comerciantes).\nPromedio: ingresos medios y gasto medio.\n\n\n\n8.1.2 Resolución con PAM (k-medoids)\n\n# 1. Cargar y limpiar datos\ndatos &lt;- read_csv(\"data/mallcustomers.csv\")\n\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Gender, Income\ndbl (3): CustomerID, Age, SpendingScore\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndatos_limpios &lt;- datos %&gt;%\n  mutate(\n    Income = as.numeric(gsub(\"[^0-9]\", \"\", Income)),\n    Gender = as.factor(Gender)\n  )\n\n# 2. Seleccionar y Escalar (Fundamental para PAM también)\ndatos_num &lt;- datos_limpios %&gt;%\n  select(Age, Income, SpendingScore)\n\ndatos_scaled &lt;- scale(datos_num)\n\nSelección del número óptimo de grupos.\n\n# Método de la Silueta para PAM\nfviz_nbclust(datos_scaled, cluster::pam, method = \"silhouette\") +\n  labs(subtitle = \"Número óptimo de clusters para PAM (Silueta)\")\n\n\n\n\n\n\n\n# Método del Codo (Elbow) para PAM\nfviz_nbclust(datos_scaled, cluster::pam, method = \"wss\") +\n  geom_vline(xintercept = 6, linetype = 2) +\n  labs(subtitle = \"Número óptimo de clusters para PAM (Codo)\")\n\n\n\n\n\n\n\n\n\nset.seed(123)\n\n# Ejecutamos PAM con k=6\n# metric = \"euclidean\" es el estándar, pero PAM permite \"manhattan\" también.\npam_res &lt;- pam(datos_scaled, k = 6, metric = \"euclidean\")\n\n# Resumen básico\nprint(pam_res)\n\nMedoids:\n      ID        Age      Income SpendingScore\n[1,]  22 -0.9914806 -1.39198127   0.882915982\n[2,]  35  0.7266085 -1.04931630  -1.401822744\n[3,]  81  1.2993049 -0.24976469   0.030979508\n[4,]  98 -0.8483065 -0.02132138  -0.007744877\n[5,] 176 -0.6335454  1.04474743   1.386332990\n[6,] 167  0.2254992  0.96859966  -1.169476433\nClustering vector:\n  [1] 1 1 2 1 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2\n [38] 1 2 1 3 1 2 1 2 1 3 4 4 4 3 4 4 3 3 3 3 3 4 3 3 4 3 3 3 4 3 3 4 4 3 3 3 3\n [75] 3 4 3 4 4 3 3 4 3 3 4 3 3 4 4 3 3 4 3 4 4 4 3 4 3 4 4 3 3 4 3 4 3 3 3 3 3\n[112] 4 4 4 4 4 3 3 3 3 4 4 4 5 4 5 6 5 6 5 6 5 4 5 6 5 6 5 4 5 6 5 4 5 6 5 6 5\n[149] 6 5 6 5 6 5 6 5 6 5 6 5 3 5 6 5 6 5 6 5 6 5 6 5 6 5 6 5 6 5 6 5 6 5 6 5 6\n[186] 5 6 5 6 5 6 5 6 5 6 5 6 5 6 5\nObjective function:\n    build      swap \n0.8095825 0.7542872 \n\nAvailable components:\n [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\"  \"isolation\" \n [6] \"clusinfo\"   \"silinfo\"    \"diss\"       \"call\"       \"data\"      \n\n\n\nfviz_cluster(pam_res, \n             data = datos_scaled,\n             palette = \"jco\",\n             ellipse.type = \"t\", # 't' asume distribución t (menos estricto que convex)\n             repel = TRUE,       # Evita solapamiento de texto\n             ggtheme = theme_classic(),\n             main = \"Clustering con PAM (K-Medoids)\"\n)\n\n\n\n\n\n\n\nFigura 8.3\n\n\n\n\n\nEvaluación de la calidad del agrupamiento mediante el gráfico de Silueta.\n\n# Visualización detallada de la Silueta\ngrafico_silueta &lt;- fviz_silhouette(pam_res, palette = \"jco\", print.summary = FALSE)\n\n# Mostrar el gráfico\nprint(grafico_silueta)\n\n\n\n\n\n\n\n\nEn la gráfica, cada barra horizontal representa a un cliente. Las barras están ordenadas por cluster (agrupamiento), mientras que la línea roja punteada señala el valor del promedio global. Si la mayoría de barras superan esta línea, entonces el clustering se considera robusto.\n¿Cuáles serían los clientes potencialmente mal clasificados?\n\n# 1. Calcular los valores de silueta individuales\nsil_values &lt;- silhouette(pam_res$cluster, dist(datos_scaled))\n\n# 2. Identificar índices con silueta negativa (mal clasificados)\nindices_negativos &lt;- which(sil_values[, \"sil_width\"] &lt; 0)\n\n# 3. Mostrar quiénes son esos clientes en el dataset original\n# (Si no hay resultados, ¡felicidades! tu clustering es muy limpio)\nclientes_problematicos &lt;- datos_limpios[indices_negativos, ]\n\nprint(\"Clientes con silueta negativa (posibles outliers):\")\n\n[1] \"Clientes con silueta negativa (posibles outliers):\"\n\nprint(clientes_problematicos)\n\n# A tibble: 5 × 5\n  CustomerID Gender   Age Income SpendingScore\n       &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;\n1          5 Female    31  17000            40\n2         43 Male      48  39000            36\n3         44 Female    31  39000            61\n4        135 Male      20  73000             5\n5        147 Male      48  77000            36\n\n# 4. Ver a qué cluster fueron asignados vs a cuál deberían ir (vecino más cercano)\ninfo_negativos &lt;- data.frame(\n  ClienteID = indices_negativos,\n  Cluster_Asignado = sil_values[indices_negativos, \"cluster\"],\n  Vecino_Cercano = sil_values[indices_negativos, \"neighbor\"], # El cluster al que \"quieren\" ir\n  Valor_Silueta = sil_values[indices_negativos, \"sil_width\"]\n)\n\nprint(info_negativos)\n\n  ClienteID Cluster_Asignado Vecino_Cercano Valor_Silueta\n1         5                1              2   -0.02425103\n2        43                2              3   -0.05441638\n3        44                1              4   -0.01059985\n4       135                6              4   -0.01934956\n5       147                6              3   -0.01341218\n\n\nPor último, el valor promedio de la anchura de la silueta nos indica, globalmente, si los grupos están separados (valor cercano a 1) o los datos no están agrupados de forma natural (valores cercanos a 0).\n\nsummary(sil_values)$avg.width\n\n[1] 0.4253104\n\n\nInterpretación de los medoides (clientes prototipo).\n\n# Extraemos los clientes reales que son los centros de cada grupo\nmedoides_reales &lt;- datos_limpios[pam_res$id.med, ]\n\n# Añadimos a qué cluster representan\nmedoides_reales$Cluster_Representado &lt;- 1:6\n\nprint(\"Los 'Clientes Arquetipo' (Medoides) de cada grupo son:\")\n\n[1] \"Los 'Clientes Arquetipo' (Medoides) de cada grupo son:\"\n\nprint(medoides_reales)\n\n# A tibble: 6 × 6\n  CustomerID Gender   Age Income SpendingScore Cluster_Representado\n       &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt;  &lt;dbl&gt;         &lt;dbl&gt;                &lt;int&gt;\n1         22 Male      25  24000            73                    1\n2         35 Female    49  33000            14                    2\n3         81 Male      57  54000            51                    3\n4         98 Female    27  60000            50                    4\n5        176 Female    30  88000            86                    5\n6        167 Male      42  86000            20                    6",
    "crumbs": [
      "Aplicaciones",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Aplicaciones</span>"
    ]
  },
  {
    "objectID": "08-apps.html#ejemplo-de-reducc.-dim.",
    "href": "08-apps.html#ejemplo-de-reducc.-dim.",
    "title": "8  Aplicaciones",
    "section": "8.2 Ejemplo de reducc. dim.",
    "text": "8.2 Ejemplo de reducc. dim.",
    "crumbs": [
      "Aplicaciones",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Aplicaciones</span>"
    ]
  },
  {
    "objectID": "09-add-resources.html",
    "href": "09-add-resources.html",
    "title": "9  Recursos adicionales",
    "section": "",
    "text": "La principal referencia práctica sobre implementación de modelos de aprendizaje máquina en R sigue siendo (Boehmke & Greenwell, 2019), tal y como hemos visto en todo este taller.\n\nLibro completo publicado en abierto.\n\nEl propio B. Boehmke mantiene un estupendo listado de recursos de aprendizaje para Ciencia de Datos. El listado de referencias sobre aprendizaje máquina es muy extenso, aunque incluye contenidos tanto en Python como en R y otros lenguajes.\nDespués de su segunda edición, An Introduction to Statistical Learning continúa siendo una de las referencias más sólidas, accesibles y completas para comprender los fundamentos de aprendizaje estadístico en los que se basan muchos de estos modelos/algoritmos, desde ese punto de vista. Como nota adicional, recalcar que el mismo libro está ahora disponible tanto en su versión para R como en otra versión para Python, lo que incrementa aún más si cabe su valor.\nPor último, Machine Learning with R (4th ed.) es otro clásico que se sigue renovando para incorporar las últimas tendencias, y también incluye numerosos ejemplos que ilustran diferentes algoritmos y métodos de ajuste y evaluación.\n\n\n\n\n\nBoehmke, B., & Greenwell, B. (2019). Hands-On Machine Learning with R (1.ª ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recursos adicionales</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Arthur, D., & Vassilvitskii, S. (2007). K-means++: The advantages of\ncareful seeding. In N. Bansal, K. Pruhs, & C. Stein (Eds.),\nProceedings of the eighteenth annual ACM-SIAM symposium\non discrete algorithms, SODA 2007, new orleans, louisiana,\nUSA, january 7-9, 2007 (pp. 1027–1035). SIAM.\n\n\nBoehmke, B., & Greenwell, B. (2019). Hands-on machine learning\nwith r (1st ed.). Chapman; Hall/CRC. https://doi.org/10.1201/9780367816377\n\n\nJames, W., G. (2021). An Introduction to\nStatistical Learning with Applications in R (2nd ed.).\nSpringer. https://www.statlearning.com/\n\n\nMaaten, L. van der, & Hinton, G. (2008). Visualizing data using\nt-SNE. Journal of Machine Learning Research, 9(Nov),\n2579–2605.\n\n\nNwanganga, F., & Chapple, M. (2020). Practical machine learning\nin r (1st ed.). John Wiley & Sons, Inc. https://www.wiley.com/en-us/Practical+Machine+Learning+in+R-p-9781119591535\n\n\nRussell, S., & Norvig, P. (2009). Artificial intelligence: A\nmodern approach (3rd ed.). Prentice Hall Press.\n\n\nSchubert, E., & Rousseeuw, P. J. (2019). Faster k-medoids\nclustering: Improving the PAM, CLARA, and CLARANS algorithms. In G.\nAmato, C. Gennaro, V. Oria, & M. Radovanović (Eds.), Similarity\nsearch and applications (pp. 171–187). Springer International\nPublishing.\n\n\nSkopal, T. (2007). Unified framework for fast exact and approximate\nsearch in dissimilarity spaces. ACM Transactions on\nDatabase Systems, 32(4), 29. https://doi.org/10.1145/1292609.1292619\n\n\nTan, P.-N., Steinbach, M. S., Karpatne, A., & Kumar, V. (2019).\nIntroduction to data mining (second edition). Pearson.\n\n\nWang, S., Sun, Y., & Bao, Z. (2020). On the efficiency of k-means\nclustering: Evaluation, optimization, and algorithm selection. Proc.\nVLDB Endow., 14(2), 163–175. https://doi.org/10.14778/3425879.3425887\n\n\nXu, R., & II, D. C. W. (2009). Clustering.\nWiley-IEEE Press. https://ieeexplore.ieee.org/book/5236612",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "B-packages.html",
    "href": "B-packages.html",
    "title": "Apéndice B — Paquetes R empleados",
    "section": "",
    "text": "Nombre\nDescripción\n\n\n\n\ndplyr\nProcesamiento y preparación de datos\n\n\nggplot2\nRepresentaciones gráficas\n\n\npatchwork\nComposición de gráficas con ggplot2\n\n\nHmisc\nHerramientas para descripción de datos\n\n\nrsample\nTécnicas de remuestreo de datos\n\n\ncaret\nAjuste, evaluación y rendimiento de modelos\n\n\nh2o\nModelos ML avanzados y distribuidos\n\n\nAmesHousing\nDataset sobre precios de viviendas",
    "crumbs": [
      "Apéndices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Paquetes R empleados</span>"
    ]
  }
]